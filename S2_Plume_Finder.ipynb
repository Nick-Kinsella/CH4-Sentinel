{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7f561c3-51d4-4476-922d-876088df12c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sentinel-2 Methane Emission Detection and Analysis Tool\n",
    "\n",
    "## Overview \n",
    "This notebook provides a comprehensive workflow for detecting and analysing methane plumes from oil and gas facilities using Sentinel-2 satellite data. It combines satellite imagery processing, wind speed analysis, and regression modelling to estimate methane emission rates accurately. Key functionalities include:\n",
    "\n",
    "1. **SWIR Analysis**: Utilises Sentinel-2's Short-Wave Infrared (SWIR) bands to highlight methane plumes by comparing active and non-active regions.\n",
    "2. **Plume Detection and Tagging**: Supports manual tagging of plume locations and segmentation for precise analysis.\n",
    "3. **Regression-Based Emission Estimation**: Employs a trained regression model to estimate methane emission rates based on plume characteristics and wind speed data.\n",
    "4. **Interactive Visualisation**: Creates interactive maps to visualise true-colour images, SWIR-derived plumes, and analysis results.\n",
    "5. **Dynamic Model Updates**: Facilitates the addition of new training data to refine the regression model for improved predictions.\n",
    "\n",
    "This tool is designed for researchers, policymakers, and environmental analysts aiming to quantify and monitor methane emissions efficiently.\n",
    "\n",
    "The section below imports the packages needed to run the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af9cfbc-e95b-4685-a7e2-19f2ccc7cf3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Core System and Numerical Operations\n",
    "import os  # For file path and system-level operations\n",
    "import numpy as np  # For numerical operations and array manipulations\n",
    "import pandas as pd  # For handling tabular data (e.g., CSV files)\n",
    "\n",
    "# File Handling and Temporary Files\n",
    "from tempfile import NamedTemporaryFile  # For creating temporary files\n",
    "\n",
    "# Machine Learning and Statistical Analysis\n",
    "from sklearn.linear_model import LinearRegression  # Regression model for methane emission estimation\n",
    "from sklearn.metrics import mean_squared_error, r2_score  # Metrics for model evaluation\n",
    "from sklearn.decomposition import PCA  # For principal component analysis (PCA)\n",
    "\n",
    "# Data Analysis and Manipulation\n",
    "import xarray as xr  # For working with multidimensional arrays (e.g., NetCDF files)\n",
    "import cdsapi  # For accessing the Copernicus Climate Data Store API\n",
    "import requests  # For making HTTP requests (e.g., downloading data)\n",
    "import openeo  # For cloud-based geospatial data processing\n",
    "\n",
    "# Geospatial Data Handling\n",
    "import geopandas as gpd  # For working with GeoJSON and vector geospatial data\n",
    "import rasterio  # For working with raster data\n",
    "from rasterio.enums import Resampling  # For resampling raster data\n",
    "from rasterio.plot import show  # For visualising raster data\n",
    "from rasterio.transform import from_origin  # For creating geospatial transformations\n",
    "from rasterio.warp import calculate_default_transform, reproject  # For reprojection of raster data\n",
    "from shapely.geometry import Point, LineString  # For geometric operations in geospatial analysis\n",
    "\n",
    "# Image Processing\n",
    "from skimage import exposure  # For adjusting image exposure and contrast\n",
    "from PIL import Image  # For basic image manipulation\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.features import geometry_mask\n",
    "\n",
    "# Mathematical and Geometric Computations\n",
    "from scipy.ndimage import label  # For segmentation and labelling of regions\n",
    "from scipy.spatial import ConvexHull  # For calculating convex hulls of shapes\n",
    "\n",
    "# Interactive Maps and Visualisation\n",
    "import folium  # For creating interactive maps\n",
    "from folium import Map, GeoJson, LayerControl, LatLngPopup  # Map features and interactions\n",
    "from folium.raster_layers import ImageOverlay  # Overlay raster images on maps\n",
    "from folium import FeatureGroup  # For grouping map layers\n",
    "import matplotlib.pyplot as plt  # For plotting and visualisation\n",
    "\n",
    "# Jupyter Notebook Integration\n",
    "from IPython.display import display as ipy_display  # For displaying outputs in notebooks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67714102-a656-4414-9e45-42c8c80460ea",
   "metadata": {},
   "source": [
    "## Connect to OpenEO\n",
    "\n",
    "The code below establishes a connection with the Copernicus openEO platform which provides a wide variety of earth observation datasets\n",
    "\n",
    "- If this does not read as 'Authorised successfully' or 'Authenticated using refresh token', then please ensure that you have completed the setup steps as outlined in section 2.3.6 of the how to guide. \n",
    "\n",
    "- If you have followed the steps in section 2.3.6 correctly and the problem persists, please look at https://dataspace.copernicus.eu/news for any information about service interruptions. \n",
    "\n",
    "- If there is no news of service problems you can raise a ticket here: https://helpcenter.dataspace.copernicus.eu/hc/en-gb/requests/new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfface4-832c-4c3b-b79c-a6886878cd83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "connection = openeo.connect(url=\"openeo.dataspace.copernicus.eu\")\n",
    "connection.authenticate_oidc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d0674c-3b9c-4ab0-aafe-0526baa82e66",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dispaly field names and select site_id. \n",
    "\n",
    "This loads the oil and gas field list. Hassi Messaoud is site 86. If you are interested in a different field, please look-up its id number. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3110af1e-a1d9-4fb8-8663-51d279eac192",
   "metadata": {},
   "outputs": [],
   "source": [
    "studysite_csv = pd.read_csv(r'C:\\GIS_Course\\Methane_Point_Detection\\Sentinel-2_Algeria_Methane\\Data\\Algerian_Oil_and_Gas_Fields.csv')\n",
    "#studysite_csv = pd.read_csv(r'C:\\GIS_Course\\Methane_Point_Detection\\Sentinel-2_Algeria_Methane\\Data\\Individual_Plume_Boundings.csv')\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(studysite_csv.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4115bf0d-be37-48b7-bed1-f3929708b174",
   "metadata": {},
   "source": [
    "# Site selection\n",
    "\n",
    "In the code box below, specify the field number we are interested in for analysis. \n",
    "\n",
    "<p style=\"text-align: center;\"><b>site_id</b> = 86</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f938e585-a992-4004-b7a6-c71fb7322bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_id = 86  # Specify the oil and gas field ID for the field you want to examine.\n",
    "\n",
    "# Retrieve the name of the field from the dataset\n",
    "field_name = studysite_csv[studysite_csv['id'] == site_id].iloc[0]['name']\n",
    "\n",
    "# Print a confirmation message\n",
    "print(f\"Site {site_id} ({field_name}) loaded correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb74f40-ba0f-4dbb-9d38-08e5b33dc11f",
   "metadata": {},
   "source": [
    "# Multi-Band Multi-Pass Analysis\n",
    "\n",
    "Varon et al. (2021) showed that methane plumes from point sources could be imaged by differencing Sentinel-2’s SWIR-1 and SWIR-2 bands. The tool runs an analysis using a  multi-band-multi-pass retrieval method: \n",
    "\n",
    "First it calculates a multi-band-single-pass calculation for both active emission and no emission dates, resulting in two datasets which are then used together for a multi-band-multi-pass method. \n",
    "The multi-band-single-pass equation is as follows: \n",
    "\n",
    "\n",
    "<div align=\"center\"><b>MBSP = cB12 - B11 / B11  </b></div>\n",
    "\n",
    "Where:\n",
    "- <b>B12</b> is the Sentinel-2 SWIR-2 band.\n",
    "- <b>B11</b> is the Sentinel-2 SWIR-1 band. \n",
    "- <b>c</b> is calculated by least-squares fitting B12 to B11 across the scene.  \n",
    "\n",
    "Once active emission and no emission scenes have been calculated, the following equation is used to calculate the multi-band-multi-pass raster. \n",
    "\n",
    "<div align=\"center\"><b>MBMP = ActiveMBSP − NoMBSP</b></div>\n",
    "\n",
    "Where:\n",
    "- <b>ActiveMBSP</b> is the multiband single pass for the active emission scene\n",
    "- <b>NoMBSP</b> is the multiband single pass for the no emission scene.  \n",
    "\n",
    "The active emission scene and no emission scene are considered in this analysis to be one satelite pass apart. To begin this process we need to determine what days have available satelite data. \n",
    "\n",
    "# Available dates for the analysis. \n",
    "\n",
    "Sentinel 2 provides data aproximately once every 2 - 3 days, so not every date you can enter into this tool is valid. The code below will tell you what dates are available to use for the oil/gas field of your choice. \n",
    "\n",
    "The one parameter you need to modify before running the code is: \n",
    "\n",
    "- <b>temporal_extent</b> = [\"2020-01-01\", \"2020-01-31\"] (change this to your chosen date range using \"YYYY-MM-DD\" format.)\n",
    "\n",
    "Once you have done this run the code and the available dates should appear below in a matter of seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feb8600-dfdd-491b-bde9-2eaa760af099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spatial_extent(site_id):\n",
    "    site = studysite_csv[studysite_csv['id'] == site_id].iloc[0]\n",
    "    return {\n",
    "        \"west\": site['west'],\n",
    "        \"south\": site['south'],\n",
    "        \"east\": site['east'],\n",
    "        \"north\": site['north']\n",
    "    }\n",
    "\n",
    "def fetch_available_dates(site_id, temporal_extent):\n",
    "    spatial_extent = get_spatial_extent(site_id)\n",
    "    catalog_url = f\"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel2/search.json?box={spatial_extent['west']}%2C{spatial_extent['south']}%2C{spatial_extent['east']}%2C{spatial_extent['north']}&sortParam=startDate&sortOrder=ascending&page=1&maxRecords=1000&status=ONLINE&dataset=ESA-DATASET&productType=L2A&startDate={temporal_extent[0]}T00%3A00%3A00Z&completionDate={temporal_extent[1]}T00%3A00%3A00Z&cloudCover=%5B0%2C{cloud_cover}%5D\"\n",
    "    response = requests.get(catalog_url)\n",
    "    response.raise_for_status()\n",
    "    catalog = response.json()\n",
    "    dates = [date.split('T')[0] for date in map(lambda x: x['properties']['startDate'], catalog['features'])]\n",
    "    return dates\n",
    "\n",
    "# Specify the the date range you want to check for available data.\n",
    "temporal_extent = [\"2020-01-01\", \"2020-01-15\"]  \n",
    "cloud_cover = 5\n",
    "\n",
    "available_dates = fetch_available_dates(site_id, temporal_extent)\n",
    "print(\"Available dates:\", available_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c953b692-eb71-4c15-b3d3-95025f77bcc4",
   "metadata": {},
   "source": [
    "## Choosing the \"Active Emission\" Date\n",
    "\n",
    "A so called active emission date must be chosen from one of the available datasets. This will be the chosen day we are looking for plumes.  \n",
    "\n",
    "Like before, the one parameter you need to modify before running the code is:\n",
    "\n",
    "<p style=\"text-align: center;\"><b>temporal_extent</b> = [\"2020-01-17\", \"2020-01-17\"]</p>\n",
    "\n",
    "Change this to your chosen date range using \"YYYY-MM-DD\" format. \n",
    "\n",
    "Please note that the temporal extent dates <b><u>MUST BE IDENTICAL</u></b> because we are only choosing a single date.\n",
    "\n",
    "If you recieve an error message of 'NoDataAvailable' then please check the list of available data above and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a709165f-4542-46da-af8e-a7199df573c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_emission(site_id, active_temporal_extent):\n",
    "    site = studysite_csv[studysite_csv['id'] == site_id].iloc[0]\n",
    "\n",
    "    active_emission = connection.load_collection(\n",
    "        \"SENTINEL2_L2A\",\n",
    "        temporal_extent=active_temporal_extent,\n",
    "        spatial_extent={\n",
    "            \"west\": site['west'],\n",
    "            \"south\": site['south'],\n",
    "            \"east\": site['east'],\n",
    "            \"north\": site['north']\n",
    "        },\n",
    "        bands=[\"B11\", \"B12\"],\n",
    "    )\n",
    "    active_emission.download(\"Sentinel-2_active_emissionMBMP.Tiff\")\n",
    "\n",
    "# Enter parameters for the active emission day\n",
    "active_temporal_extent = [\"2024-09-29\", \"2024-09-29\"]\n",
    "\n",
    "active_emission(site_id, active_temporal_extent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadc9eea-43af-400e-822f-725ce1afb0b2",
   "metadata": {},
   "source": [
    "## Choosing the \"No Emission\" Date\n",
    "\n",
    "Next we choose the no emission date using the same process. This is the dataset we will compare the \"Active Emission\" one too. The recommended choice is the satelite overpass immediately before the \"Active Emission\" one. \n",
    "\n",
    "<b>So if your active emission day is 2020-01-17, your no emission day would be 2020-01-14</b>\n",
    "\n",
    "In an ideal world, the \"No Emission\" day should contain no emissions, but in fields with a lot of activity like Hassi Messaoud, this may not be possible. Such an instance will not cause problems in most cases. The emissions for these dates will simply appear as dark clouds on the SWIR data and can be ignored in the analysis. \n",
    "\n",
    "The one parameter you need to modify before running the code is:\n",
    "\n",
    "<p style=\"text-align: center;\"><b>temporal_extent</b> = [\"2020-01-14\", \"2020-01-14\"]</p>\n",
    "\n",
    "The temporal extent dates <b><u>MUST BE IDENTICAL</u></b>\n",
    "\n",
    "If you recieve an error message of 'NoDataAvailable' then please check the list of available data above and try again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e74d95f-bc49-4233-b767-6a1daaaf68ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_emission(site_id, temporal_extent):\n",
    "    site = studysite_csv[studysite_csv['id'] == site_id].iloc[0]\n",
    "\n",
    "    no_emission = connection.load_collection(\n",
    "        \"SENTINEL2_L2A\",\n",
    "        temporal_extent=no_temporal_extent,\n",
    "        spatial_extent={\n",
    "            \"west\": site['west'],\n",
    "            \"south\": site['south'],\n",
    "            \"east\": site['east'],\n",
    "            \"north\": site['north']\n",
    "        },\n",
    "        bands=[\"B11\", \"B12\"],\n",
    "    )\n",
    "    no_emission.download(\"Sentinel-2_no_emissionMBMP.Tiff\")\n",
    "\n",
    "# Enter perameters for the active emission day\n",
    "no_temporal_extent = [\"2024-09-27\", \"2024-09-27\"]\n",
    "\n",
    "no_emission(site_id, no_temporal_extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de701594-5589-41ad-8750-5a57f70cc130",
   "metadata": {},
   "source": [
    "## Choosing a Background Satelite Image\n",
    "\n",
    "This section helps with locating the source of the emission by displaying a true colour satelite image of the oil/gas field that the data will be superimposed over. This will help distinguish between true emissions and visual spectrum observable clouds. It is recommended that you choose the same date as your active emission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9db215-2b88-4bf4-b65c-06dc75e94a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproject_to_epsg4326(data, meta):\n",
    "    \"\"\"\n",
    "    Reprojects the given raster data to EPSG:4326 and returns the updated data and metadata.\n",
    "    \"\"\"\n",
    "    target_crs = \"EPSG:4326\"\n",
    "    \n",
    "    # Calculate transform and metadata for the target CRS\n",
    "    transform, width, height = calculate_default_transform(\n",
    "        meta['crs'], target_crs, meta['width'], meta['height'], *meta['bounds']\n",
    "    )\n",
    "    \n",
    "    # Update metadata for the new projection\n",
    "    new_meta = meta.copy()\n",
    "    new_meta.update({\n",
    "        \"crs\": target_crs,\n",
    "        \"transform\": transform,\n",
    "        \"width\": width,\n",
    "        \"height\": height,\n",
    "    })\n",
    "    \n",
    "    # Prepare an in-memory array for reprojected data\n",
    "    reprojected_data = []\n",
    "    for i in range(meta['count']):\n",
    "        # Create an empty numpy array to store the reprojected data for the band\n",
    "        destination = np.empty((height, width), dtype=data[i].dtype)\n",
    "        reproject(\n",
    "            source=data[i],\n",
    "            destination=destination,\n",
    "            src_transform=meta['transform'],\n",
    "            src_crs=meta['crs'],\n",
    "            dst_transform=transform,\n",
    "            dst_crs=target_crs,\n",
    "            resampling=Resampling.nearest\n",
    "        )\n",
    "        reprojected_data.append(destination)\n",
    "    \n",
    "    return reprojected_data, new_meta\n",
    "\n",
    "def truecolour_image(site_id, temporal_extent):\n",
    "    \"\"\"\n",
    "    Downloads and reprojects Sentinel-2 true-colour images for a given site and temporal extent.\n",
    "    \"\"\"\n",
    "    site = studysite_csv[studysite_csv['id'] == site_id].iloc[0]\n",
    "\n",
    "    truecolour_image = connection.load_collection(\n",
    "        \"SENTINEL2_L2A\",\n",
    "        temporal_extent=temporal_extent,\n",
    "        spatial_extent={\n",
    "            \"west\": site['west'],\n",
    "            \"south\": site['south'],\n",
    "            \"east\": site['east'],\n",
    "            \"north\": site['north']\n",
    "        },\n",
    "        bands=[\"B02\", \"B03\", \"B04\"],\n",
    "    )\n",
    "    # Download the true colour image\n",
    "    file_path = \"Sentinel-2_truecolourMBMP.Tiff\"\n",
    "    truecolour_image.download(file_path)\n",
    "    \n",
    "    # Read the file into memory\n",
    "    with rasterio.open(file_path) as src:\n",
    "        data = [src.read(i) for i in range(1, src.count + 1)]\n",
    "        meta = src.meta.copy()\n",
    "        meta['bounds'] = src.bounds\n",
    "\n",
    "    # Reproject the data in memory\n",
    "    reprojected_data, reprojected_meta = reproject_to_epsg4326(data, meta)\n",
    "    \n",
    "    # Save the reprojected file\n",
    "    output_file = \"Sentinel-2_truecolour_reprojected.Tiff\"\n",
    "    with rasterio.open(output_file, \"w\", **reprojected_meta) as dest:\n",
    "        for i, band in enumerate(reprojected_data, start=1):\n",
    "            dest.write(band, i)\n",
    "    \n",
    "    # Print the CRS of the output file\n",
    "    with rasterio.open(output_file) as reprojected_file:\n",
    "        print(\"CRS of the reprojected file:\", reprojected_file.crs)\n",
    "\n",
    "# Enter parameters for the no emission day\n",
    "temporal_extent = active_temporal_extent\n",
    "\n",
    "truecolour_image(site_id, temporal_extent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fde51c-73cf-4816-b880-bf4715dc35aa",
   "metadata": {},
   "source": [
    "## Running Plume Visualiser Analysis\n",
    "The code below will use the satelite data to display plumes above 1,400kgh-1 in ideal conditions. Provided all the variables above have been run correctly, this next section should take moments to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418e39a4-71fd-4190-9838-2348bd4ee736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get bounds from the Oil and Gas Field bounding file\n",
    "def get_bounds(site_id, csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    site = df[df['id'] == site_id]\n",
    "    if site.empty:\n",
    "        raise ValueError(f\"Site ID {site_id} not found in the CSV file.\")\n",
    "    site = site.iloc[0]\n",
    "    return [[site['south'], site['west']], [site['north'], site['east']]]\n",
    "\n",
    "csv_path = r'C:\\GIS_Course\\Methane_Point_Detection\\Sentinel-2_Algeria_Methane\\Data\\Algerian_Oil_and_Gas_Fields.csv'\n",
    "bounds = get_bounds(site_id, csv_path)\n",
    "\n",
    "# Define file paths\n",
    "Active_Multiband = \"Sentinel-2_active_emissionMBMP.Tiff\"\n",
    "No_Multiband = \"Sentinel-2_no_emissionMBMP.Tiff\"\n",
    "output_file = \"SWIR_diff_4326.tiff\"\n",
    "masked_output_file = \"SWIR_diff_masked_urban.tiff\"\n",
    "urban_geojson = r\"C:\\GIS_Course\\Methane_Point_Detection\\Sentinel-2_Algeria_Methane\\hassi_messaoud_urban.geojson\"\n",
    "\n",
    "# Define a function for least squares fitting\n",
    "def least_squares_fit_no_intercept(x, y):\n",
    "    \"\"\"Computes least-squares scaling factor c (forcing intercept = 0).\"\"\"\n",
    "    mask = ~np.isnan(x) & ~np.isnan(y)\n",
    "    x_valid = x[mask]\n",
    "    y_valid = y[mask]\n",
    "    \n",
    "    # Compute scaling factor c as the least-squares solution to y = c * x\n",
    "    c = np.sum(x_valid * y_valid) / np.sum(x_valid ** 2)  # Least squares slope with zero intercept\n",
    "    \n",
    "    return c\n",
    "\n",
    "# Define file paths\n",
    "Active_Multiband = \"Sentinel-2_active_emissionMBMP.Tiff\"\n",
    "No_Multiband = \"Sentinel-2_no_emissionMBMP.Tiff\"\n",
    "output_file = \"SWIR_diff.tiff\"\n",
    "\n",
    "# Open datasets and perform least squares fitting\n",
    "with rasterio.open(Active_Multiband) as Active_img, rasterio.open(No_Multiband) as No_img:\n",
    "    # Read data and convert to float for safe division\n",
    "    Active_B11 = Active_img.read(1).astype(float) / 10000.0\n",
    "    Active_B12 = Active_img.read(2).astype(float) / 10000.0\n",
    "    No_B11 = No_img.read(1).astype(float) / 10000.0\n",
    "    No_B12 = No_img.read(2).astype(float) / 10000.0\n",
    "\n",
    "    # Compute scaling factor c for each pass (forcing intercept to 0)\n",
    "    c_active = least_squares_fit_no_intercept(Active_B11.flatten(), Active_B12.flatten())\n",
    "    c_no = least_squares_fit_no_intercept(No_B11.flatten(), No_B12.flatten())\n",
    "\n",
    "    # Correct Band 12 using computed c values\n",
    "    Corrected_Active_B12 = c_active * Active_B12\n",
    "    Corrected_No_B12 = c_no * No_B12\n",
    "\n",
    "    # Compute MBSP retrieval using the correct methodology equation\n",
    "    MBSP_active = c_active * (Active_B12 - Active_B11) / Active_B11\n",
    "    MBSP_no = c_no * (No_B12 - No_B11) / No_B11\n",
    "\n",
    "    # Compute MBMP difference (Final MBMP retrieval)\n",
    "    SWIR_diff = MBSP_active - MBSP_no\n",
    "\n",
    "# Reproject and save SWIR_diff to EPSG:4326\n",
    "with rasterio.open(Active_Multiband) as src:\n",
    "    target_crs = \"EPSG:4326\"\n",
    "    transform, width, height = calculate_default_transform(\n",
    "        src.crs, target_crs, src.width, src.height, *src.bounds\n",
    "    )\n",
    "    meta = src.meta.copy()\n",
    "    meta.update({\n",
    "        \"crs\": target_crs,\n",
    "        \"transform\": transform,\n",
    "        \"width\": width,\n",
    "        \"height\": height,\n",
    "        \"count\": 1,\n",
    "        \"dtype\": SWIR_diff.dtype\n",
    "    })\n",
    "    with rasterio.open(output_file, \"w\", **meta) as dest:\n",
    "        reproject(\n",
    "            source=SWIR_diff,\n",
    "            destination=rasterio.band(dest, 1),\n",
    "            src_transform=src.transform,\n",
    "            src_crs=src.crs,\n",
    "            dst_transform=transform,\n",
    "            dst_crs=target_crs,\n",
    "            resampling=Resampling.nearest\n",
    "        )\n",
    "\n",
    "# Load GeoJSON and create urban mask\n",
    "urban_areas = gpd.read_file(urban_geojson)\n",
    "with rasterio.open(output_file) as src:\n",
    "    urban_areas = urban_areas.to_crs(src.crs)\n",
    "\n",
    "    # Rasterize the urban areas\n",
    "    urban_mask = geometry_mask(\n",
    "        [feature[\"geometry\"] for feature in urban_areas.to_crs(src.crs).__geo_interface__[\"features\"]],\n",
    "        out_shape=(src.height, src.width),\n",
    "        transform=src.transform,\n",
    "        invert=True\n",
    "    )\n",
    "\n",
    "    # Apply the urban area mask to SWIR_diff\n",
    "    swir_diff = src.read(1)\n",
    "    swir_diff_masked = np.where((urban_mask) | (swir_diff == -0.0), 32768, -swir_diff)\n",
    "\n",
    "    # Save the masked SWIR_diff to a new file\n",
    "    meta = src.meta.copy()\n",
    "    meta.update(dtype=rasterio.float32, nodata=np.nan)\n",
    "    with rasterio.open(masked_output_file, \"w\", **meta) as dest:\n",
    "        dest.write(swir_diff_masked.astype(rasterio.float32), 1)\n",
    "\n",
    "# Calculate center for map using masked SWIR_diff raster bounds\n",
    "with rasterio.open(masked_output_file) as src:\n",
    "    map_bounds = src.bounds\n",
    "    center_lat = (map_bounds.top + map_bounds.bottom) / 2\n",
    "    center_lon = (map_bounds.left + map_bounds.right) / 2\n",
    "\n",
    "# Create Folium map\n",
    "m = Map(location=[center_lat, center_lon], zoom_start=10, control_scale=True)\n",
    "\n",
    "# Load the true color image\n",
    "truecolour_sat = 'Sentinel-2_truecolour_reprojected.Tiff'\n",
    "img = rasterio.open(truecolour_sat)\n",
    "blue, green, red = img.read(1), img.read(2), img.read(3)\n",
    "\n",
    "# Adjust brightness dynamically\n",
    "brightness_factor = 0.03\n",
    "blue = np.clip(blue * brightness_factor, 0, 255)\n",
    "green = np.clip(green * brightness_factor, 0, 255)\n",
    "red = np.clip(red * brightness_factor, 0, 255)\n",
    "\n",
    "# Stack bands to create RGB image\n",
    "rgb = np.dstack((red, green, blue))\n",
    "rgb = rgb / rgb.max()\n",
    "rgb = np.log1p(rgb)\n",
    "rgb = rgb / rgb.max()\n",
    "\n",
    "# Add true color image overlay\n",
    "with rasterio.open(masked_output_file) as src:\n",
    "    swir_bounds = [[src.bounds.bottom, src.bounds.left], [src.bounds.top, src.bounds.right]]\n",
    "\n",
    "truecolour_overlay = ImageOverlay(\n",
    "    name=\"Truecolour\",\n",
    "    image=rgb,\n",
    "    bounds=swir_bounds,\n",
    "    opacity=1,  # Lower opacity for blending with SWIR overlay\n",
    "    interactive=True,\n",
    "    cross_origin=False,\n",
    "    zindex=1,  # Lower zindex to place below SWIR overlay\n",
    ")\n",
    "truecolour_overlay.add_to(m)\n",
    "\n",
    "# Load and stretch SWIR_diff for visualization\n",
    "with rasterio.open(masked_output_file) as src:\n",
    "    swir_bounds = [[src.bounds.bottom, src.bounds.left], [src.bounds.top, src.bounds.right]]\n",
    "    swir_data = src.read(1)\n",
    "\n",
    "    # Mask invalid data and clip negative values\n",
    "    swir_data = np.ma.masked_invalid(swir_data)\n",
    "    swir_data = np.ma.masked_equal(swir_data, 32768)\n",
    "\n",
    "    # Calculate mean and std only for valid data\n",
    "    mean = np.nanmean(swir_data)\n",
    "    std = np.nanstd(swir_data)\n",
    "    std_factor = 2  # Stretch factor\n",
    "\n",
    "    # Calculate stretching bounds within the valid data range\n",
    "    lower_bound = max(mean - std_factor * std, swir_data.min())\n",
    "    upper_bound = min(mean + std_factor * std, swir_data.max())\n",
    "\n",
    "    # Normalize the data to [0, 1]\n",
    "    normalized_swir_data = np.clip((swir_data - lower_bound) / (upper_bound - lower_bound), 0, 1)\n",
    "\n",
    "    # Apply colormap\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    rgb_data = (cmap(normalized_swir_data.filled(0))[:, :, :3] * 255).astype(np.uint8)\n",
    "\n",
    "# Add SWIR_diff overlay to map\n",
    "swir_overlay = ImageOverlay(\n",
    "    name=\"SWIR Data\",\n",
    "    image=rgb_data,\n",
    "    bounds=swir_bounds,\n",
    "    opacity=1,  # Adjust opacity for visibility\n",
    "    interactive=True,\n",
    "    zindex=2  # Ensure SWIR overlay is above other layers\n",
    ")\n",
    "swir_overlay.add_to(m)\n",
    "\n",
    "# Add GeoJSON data as a layer group\n",
    "vector_point_path = r\"C:\\GIS_Course\\Methane_Point_Detection\\Sentinel-2_Algeria_Methane\\Data\\known_point_sources.geojson\"\n",
    "gdf = gpd.read_file(vector_point_path)\n",
    "geojson_layer = FeatureGroup(name=\"Known Point Sources\", show=False)\n",
    "GeoJson(gdf.to_json()).add_to(geojson_layer)\n",
    "geojson_layer.add_to(m)\n",
    "\n",
    "# Layer control\n",
    "LayerControl().add_to(m)\n",
    "m.add_child(LatLngPopup())\n",
    "\n",
    "# Display map\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8623c7-0247-4561-b42d-3e5922b60225",
   "metadata": {},
   "source": [
    "## Plume tagging\n",
    "\n",
    "Over an area the size of an oil and gas field, many objects can erroneously show up as methane like signals if a method like thresholding was used. These include urban areas, agriculrutal irrigation projects and new constructions. To deal with this problem we will select the plumes in the image using a manual tagging system. \n",
    "\n",
    "To do this, click on a plume somewhere along its legnth, and then copy the given latitude and longitude coordinates. \n",
    "\n",
    "Maually input plume source coordinates below in the format (latitude, longitude), for example:  \n",
    "<p style=\"text-align: center;\">(31.6887, 5.8102),  # Plume 1 (latitude, longitude)</p> <p style=\"text-align: center;\">(31.7910, 5.8263),  # Plume 2 (latitude, longitude)</p> \n",
    "\n",
    "Additional lines for more plumes can be added as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6528c93-29a2-4f84-8943-d5008410194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plume_coords = [\n",
    "    (31.7778, 5.9943),  # Plume 1 (latitude, longitude)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1567e74-955f-4dbb-b8ac-7b9c1b68671e",
   "metadata": {},
   "source": [
    "## Regression Model Development\n",
    "\n",
    "A regression model is a statistical tool used to predict a dependent variable (here, methane emission rate in kg/h) based on independent variables. It works by identifying relationships in the training data and using these to estimate outcomes for new data.\n",
    "\n",
    "To train the model, data from methane plumes with emission rates documented in peer-reviewed studies was collected (Gorroño et al., 2023; Pandey et al., 2023; Varon et al., 2021; Wang et al., 2023; Sanchez-Garcia et al., 2021). These plumes were then found using the MBMP Plume Visualiser. Each plume was measured for:\n",
    "\n",
    "- **Adjusted CS Sum**: The plume intensity in its cross-section after subtracting background values.\n",
    "- **Plume Length**: The plume's length in pixels.\n",
    "- **Wind Speed**: ERA5 reanalysis data for the wind speed at the time of observation.\n",
    "\n",
    "The regression analysis identifies how these factors relate to emission rates, allowing the model to predict methane emissions for other plumes based on their characteristics.\n",
    "\n",
    "Below are the data that was collected for the regression analysis. The data used for the model as of publication, is listed below.\n",
    "\n",
    "| Plume | Longitude  | Latitude   | Date       | Emission Rate (Q) (kg/h) | Cross Sectional Adjusted Sum | Wind Speed (m/s) | Plume Length (m) | Source                                |\n",
    "|:-----:|:----------:|:----------:|:----------:|:------------------------:|:----------------------------:|:----------------:|:----------------:|:-------------------------------------:|\n",
    "|   1   |  6.154881  |  31.805489 | 31/08/2021 |          5453           |          368.969627          |       4.37       |      18.439089   |         Gorroño et al., 2023         |\n",
    "|   2   |   5.9968   |   31.7775  | 04/01/2020 |         21000           |          5578.987974         |       3.65       |     438.813172   |         Pandey et al., 2023          |\n",
    "|   3   |   5.9053   |   31.6585  | 20/11/2019 |          8500           |          4536.07795          |       0.51       |     105.171289   | Varon et al., 2021, Pandey et al., 2023 |\n",
    "|   4   |      6      |    31.78   | 19/08/2021 |          4326           |          319.004091          |       0.92       |      19.646883   | Sanchez-Garcia et al., 2021 |\n",
    "|   5   |   5.9951   |   31.7789  | 19/08/2021 |          2160           |          342.789593          |       0.92       |      42.544095   | Sanchez-Garcia et al., 2021 |\n",
    "|   6   |   6.0107   |    31.798  | 19/08/2021 |          2757           |          365.813576          |       0.92       |       7.28011    | Sanchez-Garcia et al., 2021 |\n",
    "|   7   |   5.9055   |    31.659  | 07/01/2020 |          8240           |          4641.620257         |      10.54       |     159.154013   |          Radman et al., 2023          |\n",
    "|   8   |   5.9954   |   31.7775  | 31/01/2023 |          3400           |          0.09190             |       2.3        |      18.788      |         Carbon Mapper                 |\n",
    "|   9   |   5.9936   |   31.7774  | 29/09/2024 |          3000           |          0.07393             |       8.88       |      8.544       |         Carbon Mapper                 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa51b62b-51aa-438d-b622-a7c68dcd6c90",
   "metadata": {},
   "source": [
    "Below more example plumes can be added to improve the model, should more studies become available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eff60e-16cc-4173-817f-8e9d758da69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial dataset for regression model (Add new plumes directly here as needed)\n",
    "initial_data = {\n",
    "    \"Cross_sectional_Adjusted_Sum\": [0.060439, 0.363051, 0.567032, 0.11072, 0.048257, 0.082167, 0.681818, 0.091896, 0.073934],\n",
    "    \"Wind_speed\": [4.45, 3.65, 0.49, 0.96, 0.96, 0.96, 1.44, 2.3, 8.88],\n",
    "    \"Plume_length\": [17.804494, 155.878158, 102.591423, 43.174066, 12.083046, 14.422205, 148.761554, 18.788294, 8.544004],\n",
    "    \"Emission_rate_kg_h\": [5453, 21000, 8500, 4326, 2160, 2757, 8240, 3400, 3000],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf9ea84-33c0-40b7-a932-2709086f888a",
   "metadata": {},
   "source": [
    "## Detemining wind speed\n",
    "\n",
    "Wind speed is a crucial factor in determining emission rate. This next code box determines the wind speed on the \"Active Emission\" date as part of the gas flux calculation using the ERA5 API. Several warning messages will appear but these can be ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0feeddb-5d92-4834-b681-d64679f6a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract bounding box and calculate center from site_id\n",
    "def get_location_from_site_id(site_id, csv_path):\n",
    "    \"\"\"\n",
    "    Extract center latitude and longitude for a site based on site_id.\n",
    "\n",
    "    Args:\n",
    "    - site_id (int): The ID of the site to extract.\n",
    "    - csv_path (str): Path to the CSV containing site boundaries.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary with latitude and longitude of the center.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    site = df[df['id'] == site_id]\n",
    "    if site.empty:\n",
    "        raise ValueError(f\"Site ID {site_id} not found in the CSV file.\")\n",
    "    site = site.iloc[0]\n",
    "    center_lat = (site['south'] + site['north']) / 2\n",
    "    center_lon = (site['west'] + site['east']) / 2\n",
    "    return {'latitude': center_lat, 'longitude': center_lon}\n",
    "\n",
    "# Get the location for the ERA5 data request\n",
    "location = get_location_from_site_id(site_id, csv_path)\n",
    "\n",
    "# Initialize the CDS API client\n",
    "c = cdsapi.Client()\n",
    "\n",
    "# Define parameters for the data request\n",
    "# Extract the start date from active_temporal_extent and assign it to date\n",
    "date = active_temporal_extent[0]  # Use the first element as the single date\n",
    "\n",
    "# Now the variable 'date' can be used with the other API\n",
    "\n",
    "# date = active_temporal_extent (update this somehow so that no input is needed in this box)\n",
    "\n",
    "# Retrieve ERA5 data and store it in a temporary file\n",
    "with NamedTemporaryFile(suffix='.nc') as tmp_file:\n",
    "    result = c.retrieve(\n",
    "        'reanalysis-era5-single-levels',\n",
    "        {\n",
    "            'product_type': 'reanalysis',\n",
    "            'variable': ['10m_u_component_of_wind', '10m_v_component_of_wind'],\n",
    "            'year': date.split('-')[0],\n",
    "            'month': date.split('-')[1],\n",
    "            'day': date.split('-')[2],\n",
    "            'time': ['10:00'],  # Specify time of interest\n",
    "            'format': 'netcdf',  # NetCDF format\n",
    "            'area': [\n",
    "                location['latitude'] + 0.25, location['longitude'] - 0.25,\n",
    "                location['latitude'] - 0.25, location['longitude'] + 0.25,\n",
    "            ],  # Small bounding box around the location\n",
    "        }\n",
    "    )\n",
    "    # Download data to the temporary file\n",
    "    result.download(tmp_file.name)\n",
    "    \n",
    "    # Load the dataset with xarray\n",
    "    ds = xr.open_dataset(tmp_file.name)\n",
    "\n",
    "# Extract u and v components\n",
    "u10 = ds['u10'].sel(latitude=location['latitude'], longitude=location['longitude'], method='nearest')\n",
    "v10 = ds['v10'].sel(latitude=location['latitude'], longitude=location['longitude'], method='nearest')\n",
    "\n",
    "# Calculate wind speed\n",
    "wind_speed = np.sqrt(u10**2 + v10**2)\n",
    "\n",
    "# Handle single timestep case\n",
    "if 'time' in u10.dims:\n",
    "    # Multiple timesteps (not likely in this case since we specified 10:00 only)\n",
    "    for time, speed in zip(u10.time.values, wind_speed.values):\n",
    "        print(f\"{time}: Wind Speed = {speed:.2f} m/s\")\n",
    "else:\n",
    "    # Single timestep\n",
    "    wind_speed_value = wind_speed.values.item()  # Convert array to scalar\n",
    "    print(f\"Wind Speed at 10:00 on {date}: {wind_speed_value:.2f} m/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3930168-7e93-4e4c-9f83-011f6f6ea8c7",
   "metadata": {},
   "source": [
    "## Running the tagged plume analysis\n",
    "\n",
    "The next code box analyses methane plumes we tagged earlier and provides the following information:\n",
    "\n",
    "- **Plume Insights**: Locations, sizes, and predicted methane emission rates (kg/h) visualised on an interactive map and summarised in a table.\n",
    "- **Model Evaluation**: Details on the regression model used to estimate emissions, including its performance metrics (e.g., R² and MSE).\n",
    "- **Interactive Visualisation**: A map with SWIR data overlays, plume boundaries, and tooltips for detailed exploration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8ddc9d-9dfa-4b93-a935-df53d8ceb5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "swir_diff_path = r'C:\\GIS_Course\\Methane_Point_Detection\\Sentinel-2_Algeria_Methane\\SWIR_diff_masked_urban.tiff'\n",
    "\n",
    "# Open the TIFF file\n",
    "with rasterio.open(swir_diff_path) as tiff_file:\n",
    "    raster_data = tiff_file.read(1)  # Read the first band\n",
    "    bounds = tiff_file.bounds\n",
    "    transform = tiff_file.transform\n",
    "\n",
    "    # Define nodata_value properly\n",
    "    nodata_value = tiff_file.nodata  # Extract from metadata if available\n",
    "    if nodata_value is None:\n",
    "        nodata_value = 32768  # Use a default or known invalid value\n",
    "\n",
    "# Mask nodata values\n",
    "masked_data = np.ma.masked_equal(raster_data, nodata_value)\n",
    "\n",
    "# Calculate statistical values\n",
    "# Mean and standard deviation for normalization\n",
    "mean = np.nanmean(masked_data)\n",
    "std = np.nanstd(masked_data)\n",
    "std_factor = 2  # Match the first code's normalization logic\n",
    "\n",
    "# Dynamically calculate bounds with valid range consideration\n",
    "lower_bound = max(mean - std_factor * std, masked_data.min())\n",
    "upper_bound = min(mean + std_factor * std, masked_data.max())\n",
    "\n",
    "# Normalize and clip the data\n",
    "normalized_data = (masked_data - lower_bound) / (upper_bound - lower_bound)\n",
    "normalized_data = np.clip(normalized_data, 0, 1)\n",
    "\n",
    "# Calculate the median value of the dataset\n",
    "dataset_median_value = np.ma.median(masked_data)\n",
    "\n",
    "# Define helper functions\n",
    "def get_raster_center(tiff_path):\n",
    "    with rasterio.open(tiff_path) as tiff_file:\n",
    "        bounds = tiff_file.bounds\n",
    "        center_lat = (bounds.top + bounds.bottom) / 2\n",
    "        center_lon = (bounds.left + bounds.right) / 2\n",
    "    return center_lat, center_lon\n",
    "\n",
    "def calculate_plume_width_pixels(plume_pixels, perp_direction):\n",
    "    perp_vector = np.array(perp_direction)\n",
    "    perp_vector = perp_vector / np.linalg.norm(perp_vector)\n",
    "    projections = plume_pixels @ perp_vector\n",
    "    return projections.max() - projections.min()  # Explicitly calculate the range\n",
    "\n",
    "def bresenham_line(x0, y0, x1, y1):\n",
    "    points = []\n",
    "    dx = abs(x1 - x0)\n",
    "    dy = abs(y1 - y0)\n",
    "    sx = 1 if x0 < x1 else -1\n",
    "    sy = 1 if y0 < y1 else -1\n",
    "    err = dx - dy\n",
    "\n",
    "    while True:\n",
    "        points.append((x0, y0))\n",
    "        if x0 == x1 and y0 == y1:\n",
    "            break\n",
    "        e2 = err * 2\n",
    "        if e2 > -dy:\n",
    "            err -= dy\n",
    "            x0 += sx\n",
    "        if e2 < dx:\n",
    "            err += dx\n",
    "            y0 += sy\n",
    "    return points\n",
    "\n",
    "def get_line_pixel_values(start, end, masked_data):\n",
    "    line_pixels = bresenham_line(int(start[0]), int(start[1]), int(end[0]), int(end[1]))\n",
    "    pixel_values = [masked_data[row, col] for row, col in line_pixels if 0 <= row < masked_data.shape[0] and 0 <= col < masked_data.shape[1]]\n",
    "    return pixel_values\n",
    "\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "def calculate_plume_length(plume_pixels):\n",
    "    \"\"\"\n",
    "    Calculate the length of a plume using the maximum distance between points on its convex hull.\n",
    "\n",
    "    Args:\n",
    "    - plume_pixels: A 2D numpy array where each row represents a pixel's coordinates (row, column).\n",
    "\n",
    "    Returns:\n",
    "    - Plume length in pixels (float).\n",
    "    \"\"\"\n",
    "    if len(plume_pixels) < 2:\n",
    "        return 0  # Length is zero if there are fewer than two points\n",
    "\n",
    "    # Calculate the convex hull of the plume region\n",
    "    hull = ConvexHull(plume_pixels)\n",
    "\n",
    "    # Extract the points on the hull\n",
    "    hull_points = plume_pixels[hull.vertices]\n",
    "\n",
    "    # Compute the maximum pairwise distance between hull points\n",
    "    max_distance = pdist(hull_points).max()\n",
    "\n",
    "    return max_distance\n",
    "\n",
    "def count_line_pixels(start, end):\n",
    "    line_pixels = bresenham_line(int(start[0]), int(start[1]), int(end[0]), int(end[1]))\n",
    "    return len(line_pixels)\n",
    "\n",
    "def analyze_plume_with_cross_section_sum(masked_data, plume_coords, transform, initial_center):\n",
    "    plume_map = folium.Map(location=initial_center, zoom_start=11, control_scale=True)\n",
    "    plume_results = []\n",
    "    labeled_array, _ = label(masked_data > np.percentile(masked_data.compressed(), 85))\n",
    "\n",
    "    for i, (lat, lon) in enumerate(plume_coords):\n",
    "        try:\n",
    "            row, col = rasterio.transform.rowcol(transform, lon, lat)\n",
    "            row, col = int(row), int(col)\n",
    "            plume_label = labeled_array[row, col]\n",
    "            if plume_label == 0:\n",
    "                plume_results.append({\"Plume\": i + 1, \"Location (lat, lon)\": (lat, lon), \"Status\": \"No plume detected\"})\n",
    "                continue\n",
    "\n",
    "            plume_region = labeled_array == plume_label\n",
    "            plume_pixels = np.column_stack(np.where(plume_region))\n",
    "            pca = PCA(n_components=2)\n",
    "            pca.fit(plume_pixels)\n",
    "            perp_direction = [-pca.components_[0, 1], pca.components_[0, 0]]\n",
    "\n",
    "            plume_width_pixels = calculate_plume_width_pixels(plume_pixels, perp_direction)\n",
    "\n",
    "            centroid = plume_pixels.mean(axis=0)\n",
    "            perp_line_coords = [\n",
    "                (centroid[0] - perp_direction[0] * plume_width_pixels / 2, centroid[1] - perp_direction[1] * plume_width_pixels / 2),\n",
    "                (centroid[0] + perp_direction[0] * plume_width_pixels / 2, centroid[1] + perp_direction[1] * plume_width_pixels / 2),\n",
    "            ]\n",
    "\n",
    "            line_pixel_values = get_line_pixel_values(perp_line_coords[0], perp_line_coords[1], masked_data)\n",
    "            num_intersecting_pixels = count_line_pixels(perp_line_coords[0], perp_line_coords[1])\n",
    "\n",
    "            pixel_value_sum = sum(line_pixel_values)\n",
    "            adjusted_sum = pixel_value_sum - (dataset_median_value * num_intersecting_pixels)\n",
    "\n",
    "            perp_line_latlon = [\n",
    "                rasterio.transform.xy(transform, int(pt[0]), int(pt[1])) for pt in perp_line_coords\n",
    "            ]\n",
    "            folium.PolyLine(\n",
    "                locations=[(lat, lon) for lon, lat in perp_line_latlon],\n",
    "                color=\"red\",\n",
    "                weight=2,\n",
    "                opacity=1,\n",
    "                tooltip=f\"Plume {i + 1} Width Measurement\",\n",
    "            ).add_to(plume_map)\n",
    "\n",
    "            hull = ConvexHull(plume_pixels)\n",
    "            hull_coords = [(plume_pixels[vertex][0], plume_pixels[vertex][1]) for vertex in hull.vertices]\n",
    "            hull_latlon = [rasterio.transform.xy(transform, int(pt[0]), int(pt[1])) for pt in hull_coords]\n",
    "            folium.Polygon(\n",
    "                locations=[(lat, lon) for lon, lat in hull_latlon],\n",
    "                color=\"green\",\n",
    "                weight=3,\n",
    "                fill=False,\n",
    "                opacity=1,\n",
    "                popup=f\"Plume {i + 1} region\",\n",
    "            ).add_to(plume_map)\n",
    "\n",
    "            # Calculate plume length\n",
    "            plume_length = calculate_plume_length(plume_pixels)\n",
    "\n",
    "            plume_results.append({\n",
    "                \"Plume\": i + 1,\n",
    "                \"Location (lat, lon)\": (lat, lon),\n",
    "                \"Intersecting Pixels\": num_intersecting_pixels,\n",
    "                \"Pixel Value Sum\": pixel_value_sum,\n",
    "                \"Adjusted Sum\": adjusted_sum,\n",
    "                \"Plume Length (pixels)\": plume_length  # Added plume length\n",
    "            })\n",
    "        except Exception as e:\n",
    "            plume_results.append({\"Plume\": i + 1, \"Location (lat, lon)\": (lat, lon), \"Status\": f\"Error: {e}\"})\n",
    "    return plume_results, plume_map\n",
    "\n",
    "def add_swir_data_to_map(map_object, tiff_path):\n",
    "    with rasterio.open(tiff_path) as tiff_file:\n",
    "        swir_data = tiff_file.read(1)\n",
    "        bounds = tiff_file.bounds\n",
    "        nodata_value = 32768.0  # Use the known invalid value from your data\n",
    "\n",
    "    masked_data = np.ma.masked_equal(swir_data, nodata_value)\n",
    "    mean, std = np.nanmean(masked_data), np.nanstd(masked_data)\n",
    "    lower_bound, upper_bound = mean - std_factor * std, mean + std_factor * std\n",
    "    normalized_data = (masked_data - lower_bound) / (upper_bound - lower_bound)\n",
    "    normalized_data = np.clip(normalized_data, 0, 1)\n",
    "\n",
    "    cmap = plt.get_cmap(\"viridis\")\n",
    "    swir_rgb = (cmap(normalized_data)[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_bounds = [[bounds.bottom, bounds.left], [bounds.top, bounds.right]]\n",
    "\n",
    "    # Add SWIR overlay with same properties as Truecolour\n",
    "    swir_overlay = ImageOverlay(\n",
    "        name=\"SWIR Data\",\n",
    "        image=swir_rgb,\n",
    "        bounds=image_bounds,\n",
    "        opacity=1,  # Match Truecolour opacity\n",
    "        interactive=True,  # Allow user interaction\n",
    "        cross_origin=False,  # Prevent security issues\n",
    "        zindex=2  # Place above Truecolour\n",
    "    )\n",
    "    swir_overlay.add_to(map_object)\n",
    "\n",
    "# Convert the dataset into a DataFrame\n",
    "model_df = pd.DataFrame(initial_data)\n",
    "\n",
    "# Function to fit and update the regression model using Model\n",
    "def update_model(df):\n",
    "    \"\"\"\n",
    "    Update the regression model based on the current dataset.\n",
    "\n",
    "    Args:\n",
    "    - df: DataFrame containing the plume data.\n",
    "\n",
    "    Returns:\n",
    "    - Updated regression model parameters as a dictionary.\n",
    "    \"\"\"\n",
    "    # Prepare features (X) and target (y)\n",
    "    X = df[[\"Cross_sectional_Adjusted_Sum\", \"Wind_speed\", \"Plume_length\"]]\n",
    "    y = df[\"Emission_rate_kg_h\"]\n",
    "\n",
    "    # Fit Model 2 (simple linear regression)\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(X, y)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = reg.predict(X)\n",
    "    print(\"Model Evaluation:\")\n",
    "    print(f\"Mean Squared Error (MSE): {mean_squared_error(y, y_pred):.2f}\")\n",
    "    print(f\"R-squared (R²): {r2_score(y, y_pred):.2f}\")\n",
    "\n",
    "    # Plot actual vs predicted emission rates\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y, y_pred, color=\"blue\", label=\"Predicted vs Actual\")\n",
    "    plt.plot([min(y), max(y)], [min(y), max(y)], color=\"red\", label=\"Ideal Fit Line\")\n",
    "    plt.xlabel(\"Actual Emission Rate (kg/h)\")\n",
    "    plt.ylabel(\"Predicted Emission Rate (kg/h)\")\n",
    "    plt.title(\"Regression Analysis: Actual vs Predicted\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Return updated model parameters\n",
    "    return {\n",
    "        \"intercept\": 2000,  # Force the intercept to 2000\n",
    "        \"CS_Sum_coef\": reg.coef_[0],\n",
    "        \"Wind_speed_coef\": reg.coef_[1],\n",
    "        \"Plume_length_coef\": reg.coef_[2],\n",
    "    }\n",
    "\n",
    "# Update the model with the initial data\n",
    "model_params = update_model(model_df)\n",
    "\n",
    "# Get the center of the SWIR TIFF\n",
    "center_coords = get_raster_center(swir_diff_path)\n",
    "\n",
    "# Perform plume analysis, centering the map on the SWIR TIFF\n",
    "plume_analysis_results, plume_map = analyze_plume_with_cross_section_sum(masked_data, plume_coords, transform, center_coords)\n",
    "\n",
    "# Add wind speed to each plume analysis result\n",
    "for plume in plume_analysis_results:\n",
    "    if \"Adjusted Sum\" in plume:\n",
    "        adjusted_sum = plume[\"Adjusted Sum\"]\n",
    "        plume_length = plume[\"Plume Length (pixels)\"]\n",
    "        emission_rate = (\n",
    "            model_params[\"intercept\"]\n",
    "            + model_params[\"CS_Sum_coef\"] * adjusted_sum\n",
    "            + model_params[\"Wind_speed_coef\"] * wind_speed_value\n",
    "            + model_params[\"Plume_length_coef\"] * plume_length\n",
    "        )\n",
    "        plume[\"Predicted Emission Rate (kg/h)\"] = emission_rate\n",
    "\n",
    "# Convert updated results to a DataFrame\n",
    "plume_df = pd.DataFrame(plume_analysis_results)\n",
    "plume_df.set_index(\"Plume\", inplace=True)\n",
    "\n",
    "# Display updated DataFrame with predicted emission rates\n",
    "print(f\"Median value of the dataset: {dataset_median_value}\")\n",
    "print(\"Plume Analysis Results with Predicted Emission Rates:\")\n",
    "print(plume_df)\n",
    "\n",
    "# Load the true color image\n",
    "truecolour_sat = 'Sentinel-2_truecolour_reprojected.Tiff'\n",
    "img = rasterio.open(truecolour_sat)\n",
    "blue, green, red = img.read(1), img.read(2), img.read(3)\n",
    "\n",
    "# Adjust brightness dynamically\n",
    "brightness_factor = 0.03\n",
    "blue = np.clip(blue * brightness_factor, 0, 255)\n",
    "green = np.clip(green * brightness_factor, 0, 255)\n",
    "red = np.clip(red * brightness_factor, 0, 255)\n",
    "\n",
    "# Stack bands to create RGB image\n",
    "rgb = np.dstack((red, green, blue))\n",
    "rgb = rgb / rgb.max()\n",
    "rgb = np.log1p(rgb)\n",
    "rgb = rgb / rgb.max()\n",
    "\n",
    "# Add true color image overlay\n",
    "truecolour_overlay = ImageOverlay(\n",
    "    name= \"Truecolour\",\n",
    "    image=rgb,\n",
    "    bounds=swir_bounds,\n",
    "    opacity=1,  # Lower opacity for blending with SWIR overlay\n",
    "    interactive=True,\n",
    "    cross_origin=False,\n",
    "    zindex=1,  # Lower zindex to place below SWIR overlay\n",
    ")\n",
    "truecolour_overlay.add_to(plume_map)\n",
    "\n",
    "# Add SWIR overlay to the map\n",
    "add_swir_data_to_map(plume_map, swir_diff_path)\n",
    "\n",
    "# Add a layer control to toggle map layers\n",
    "LayerControl().add_to(plume_map)\n",
    "\n",
    "# Display the map with updated analysis\n",
    "display(plume_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eb08a2-3fac-4cb4-9b44-966e8a6414d2",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. **Gorroño, J., Varon, D.J., Irakulis-Loitxate, I. and Guanter, L., 2022.** Understanding the potential of Sentinel-2 for monitoring methane point emissions. *Atmospheric Measurement Techniques Discussions, 2022*, pp.1-25.\n",
    "\n",
    "2. **Pandey, S., van Nistelrooij, M., Maasakkers, J.D., Sutar, P., Houweling, S., Varon, D.J., Tol, P., Gains, D., Worden, J. and Aben, I., 2023.** Daily detection and quantification of methane leaks using Sentinel-3: a tiered satellite observation approach with Sentinel-2 and Sentinel-5p. *Remote Sensing of Environment, 296*, p.113716.\n",
    "\n",
    "3. **Radman, A., Mahdianpari, M., Varon, D.J. and Mohammadimanesh, F., 2023.** S2MetNet: A novel dataset and deep learning benchmark for methane point source quantification using Sentinel-2 satellite imagery. *Remote Sensing of Environment, 295*, p.113708.\n",
    "\n",
    "4. **Varon, D.J., Jervis, D., McKeever, J., Spence, I., Gains, D. and Jacob, D.J., 2020.** High-frequency monitoring of anomalous methane point sources with multispectral Sentinel-2 satellite observations. *Atmospheric Measurement Techniques Discussions, 2020*, pp.1-21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23831705-ad01-4b39-a33d-44c76f1769cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
