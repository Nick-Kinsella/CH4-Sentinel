{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7f561c3-51d4-4476-922d-876088df12c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sentinel-2 Plume Finder Tool\n",
    "\n",
    "## Overview \n",
    "This notebook provides a comprehensive workflow for detecting and analysing methane plumes from oil and gas facilities using Sentinel-2 satellite data. It combines satellite imagery processing, wind speed analysis, and regression modelling to estimate methane emission rates accurately. Key functionalities include:\n",
    "\n",
    "1. **SWIR Analysis**: Uses Sentinel-2's Short-Wave Infrared (SWIR) bands to detect methane plumes.\n",
    "2. **Plume Detection and Tagging**: Uses user driven tagging of plume locations.\n",
    "3. **Regression-Based Emission Estimation**: Employs a XGBoost regression model to estimate methane emission rates based on plume characteristics and wind speed data.\n",
    "4. **Dynamic Model Updates**: Facilitates the addition of new training data to refine the XGBoost model for improved predictions.\n",
    "5. **Interactive Visualisation**: Creates an interactive map to visualise SWIR-derived plumes, and provides an estimate of their emission rates in kg/h.\n",
    "\n",
    "This tool is designed for researchers, policymakers, and environmental analysts aiming to quantify and monitor methane emissions quickly.\n",
    "\n",
    "The section below imports the packages needed to run the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af9cfbc-e95b-4685-a7e2-19f2ccc7cf3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Connecting to Sentinel-2 data\n",
    "import openeo\n",
    "\n",
    "# Available Date finder imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# SWIR and Truecolour processing imports\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.features import geometry_mask\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "\n",
    "# Interactive Maps and Visualisation\n",
    "import folium  # For creating interactive maps\n",
    "from folium import Map, LayerControl, LatLngPopup, Rectangle  # Map features and interactions\n",
    "from folium.raster_layers import ImageOverlay  # Overlay raster images on maps\n",
    "from folium import FeatureGroup  # For grouping map layers\n",
    "import matplotlib.pyplot as plt  # For plotting and visualisation\n",
    "\n",
    "# Wind Speed imports\n",
    "import cdsapi \n",
    "from tempfile import NamedTemporaryFile  \n",
    "import xarray as xr\n",
    "\n",
    "# Plume analysis imports\n",
    "from scipy.ndimage import label  # For segmentation and labelling of regions\n",
    "from scipy.spatial import ConvexHull  # For calculating convex hulls of shapes\n",
    "from scipy.spatial.distance import pdist\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "# Imports related to predictive model  \n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67714102-a656-4414-9e45-42c8c80460ea",
   "metadata": {},
   "source": [
    "## Connect to OpenEO\n",
    "\n",
    "The code below establishes a connection with the Copernicus openEO platform which provides a wide variety of earth observation datasets\n",
    "\n",
    "- If this does not read as 'Authorised successfully' or 'Authenticated using refresh token', then please ensure that you have completed the setup steps as outlined in section 2.3.6 of the how to guide. \n",
    "\n",
    "- If you have followed the steps in section 2.3.6 correctly and the problem persists, please look at https://dataspace.copernicus.eu/news for any information about service interruptions. \n",
    "\n",
    "- If there is no news of service problems you can raise a ticket here: https://helpcenter.dataspace.copernicus.eu/hc/en-gb/requests/new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfface4-832c-4c3b-b79c-a6886878cd83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "connection = openeo.connect(url=\"openeo.dataspace.copernicus.eu\")\n",
    "connection.authenticate_oidc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d0674c-3b9c-4ab0-aafe-0526baa82e66",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dispaly Field Names\n",
    "\n",
    "This loads the oil and gas field list. Hassi Messaoud is site 86. If you are interested in a different field, please look-up its id number. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3110af1e-a1d9-4fb8-8663-51d279eac192",
   "metadata": {},
   "outputs": [],
   "source": [
    "studysite_csv = pd.read_csv(r'C:\\GIS_Course\\Methane_Point_Detection\\Sentinel-2_Algeria_Methane\\Data\\Algerian_Oil_and_Gas_Fields.csv')\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(studysite_csv.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4115bf0d-be37-48b7-bed1-f3929708b174",
   "metadata": {},
   "source": [
    "# Site selection\n",
    "\n",
    "In the code box below, specify the field number we are interested in for analysis. \n",
    "\n",
    "<p style=\"text-align: center;\"><b>site_id</b> = 86</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f938e585-a992-4004-b7a6-c71fb7322bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_id = 86  # Specify the oil and gas field ID for the field you want to examine.\n",
    "\n",
    "# Retrieve the name of the field from the dataset\n",
    "field_name = studysite_csv[studysite_csv['id'] == site_id].iloc[0]['name']\n",
    "\n",
    "# Confirmation message\n",
    "print(f\"Site {site_id} ({field_name}) loaded correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb74f40-ba0f-4dbb-9d38-08e5b33dc11f",
   "metadata": {},
   "source": [
    "# Multi-Band Multi-Pass Analysis\n",
    "\n",
    "Varon et al. (2021) showed that methane plumes from point sources could be imaged by differencing Sentinel-2’s SWIR-1 and SWIR-2 bands. The tool runs an analysis using a  multi-band-multi-pass retrieval method: \n",
    "\n",
    "First it calculates a multi-band-single-pass calculation for both active emission and no emission dates, resulting in two datasets which are then used together for a multi-band-multi-pass method. \n",
    "The multi-band-single-pass equation is as follows: \n",
    "\n",
    "\n",
    "$$ MBSP = \\frac{B12 - B11}{B11} $$\n",
    "\n",
    "\n",
    "Where:\n",
    "- $B12$ is the Sentinel-2 SWIR band 12.\n",
    "- $B11$ is the Sentinel-2 SWIR band 11.\n",
    "\n",
    "Once active emission and no emission scenes have been calculated, the following equation is used to calculate the multi-band-multi-pass raster. \n",
    "\n",
    "$$ MBMP = ActiveMBSP − NoMBSP $$\n",
    "\n",
    "Where:\n",
    "- $ActiveMBSP$ is the multiband single pass for the active emission scene\n",
    "- $NoMBSP$ is the multiband single pass for the no emission scene.\n",
    "\n",
    "The active emission scene and no emission scene are considered in this analysis to be one satellite pass apart unless there is a large amount of interference from features such as clouds or other plumes, in which case an earlier date should be selected.\n",
    "\n",
    "A final step in this analysis that has been added to scale the MBMP dataset mean to zero and all other valid pixel values by that amount. This has been done to account for seasonal variations in solar radiation levels that may affect the measurements of this tool. \n",
    "\n",
    "To begin this process we need to determine what days have available satellite  data. \n",
    "\n",
    "# Available dates for the analysis. \n",
    "\n",
    "Sentinel 2 provides data aproximately once every 2 - 3 days, so not every date you can input is valid. The code below will tell you what dates are available to use for the oil/gas field of your choice. \n",
    "\n",
    "The one parameter you need to modify before running the code is: \n",
    "\n",
    "- <b>temporal_extent</b> = [\"2020-01-01\", \"2020-01-31\"] (change this to your chosen date range using \"YYYY-MM-DD\" format.)\n",
    "\n",
    "Once you have done this run the code and the available dates should appear below in a matter of seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feb8600-dfdd-491b-bde9-2eaa760af099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the date range you want to check for available data.\n",
    "temporal_extent = [\"2020-12-20\", \"2021-01-31\"] \n",
    "\n",
    "def get_spatial_extent(site_id):\n",
    "    site = studysite_csv[studysite_csv['id'] == site_id].iloc[0]\n",
    "    return {\n",
    "        \"west\": site['west'],\n",
    "        \"south\": site['south'],\n",
    "        \"east\": site['east'],\n",
    "        \"north\": site['north']\n",
    "    }\n",
    "\n",
    "def fetch_available_dates(site_id, temporal_extent):\n",
    "    spatial_extent = get_spatial_extent(site_id)\n",
    "    catalog_url = f\"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel2/search.json?box={spatial_extent['west']}%2C{spatial_extent['south']}%2C{spatial_extent['east']}%2C{spatial_extent['north']}&sortParam=startDate&sortOrder=ascending&page=1&maxRecords=1000&status=ONLINE&dataset=ESA-DATASET&productType=L2A&startDate={temporal_extent[0]}T00%3A00%3A00Z&completionDate={temporal_extent[1]}T00%3A00%3A00Z&cloudCover=%5B0%2C{cloud_cover}%5D\"\n",
    "    response = requests.get(catalog_url)\n",
    "    response.raise_for_status()\n",
    "    catalog = response.json()\n",
    "    dates = [date.split('T')[0] for date in map(lambda x: x['properties']['startDate'], catalog['features'])]\n",
    "    return dates\n",
    " \n",
    "cloud_cover = 5 # Specifies that only scenes with >5% cloud cover are chosen\n",
    "\n",
    "available_dates = fetch_available_dates(site_id, temporal_extent)\n",
    "print(\"Available dates:\", available_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c953b692-eb71-4c15-b3d3-95025f77bcc4",
   "metadata": {},
   "source": [
    "## Choosing the \"Active Emission\" Date\n",
    "\n",
    "A so called active emission date must be chosen from one of the available datasets. This will be the chosen day we are looking for plumes.  \n",
    "\n",
    "Like before, the one parameter you need to modify before running the code is:\n",
    "\n",
    "<p style=\"text-align: center;\"><b>temporal_extent</b> = [\"2020-01-17\", \"2020-01-17\"]</p>\n",
    "\n",
    "Change this to your chosen date range using \"YYYY-MM-DD\" format. \n",
    "\n",
    "Please note that the temporal extent dates <b><u>MUST BE IDENTICAL</u></b> because we are only choosing a single date.\n",
    "\n",
    "If you recieve an error message of 'NoDataAvailable' then please check the list of available data above and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a709165f-4542-46da-af8e-a7199df573c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_temporal_extent = [\"2019-11-20\", \"2019-11-20\"] # Enter parameters for the active emission day\n",
    "\n",
    "# Variable to store SZA\n",
    "sun_zenith_mean_value = None\n",
    "\n",
    "def active_emission(site_id, active_temporal_extent):\n",
    "    site = studysite_csv[studysite_csv['id'] == site_id].iloc[0]\n",
    "\n",
    "    active_emission = connection.load_collection(\n",
    "        \"SENTINEL2_L2A\",\n",
    "        temporal_extent=active_temporal_extent,\n",
    "        spatial_extent={\n",
    "            \"west\": site['west'],\n",
    "            \"south\": site['south'],\n",
    "            \"east\": site['east'],\n",
    "            \"north\": site['north']\n",
    "        },\n",
    "        bands=[\"B11\", \"B12\", \"sunZenithAngles\", \"viewZenithMean\"],\n",
    "    )\n",
    "    \n",
    "    filename = \"Sentinel-2_active_emissionMBMP.Tiff\"\n",
    "    active_emission.download(filename)\n",
    "    \n",
    "    compute_mean_values(filename)\n",
    "\n",
    "def compute_mean_values(filename):\n",
    "    global sun_zenith_mean_value, view_zenith_mean_value\n",
    "    \n",
    "    with rasterio.open(filename) as dataset:\n",
    "        bands = dataset.read()\n",
    "        \n",
    "        # Assuming sunZenithAngles is in band 3 and viewZenithMean is in band 4 (0-based index)\n",
    "        sun_zenith_mean_value = np.nanmean(bands[2])\n",
    "        print(f\"Mean Sun Zenith Angle: {sun_zenith_mean_value}\")\n",
    "\n",
    "active_emission(site_id, active_temporal_extent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadc9eea-43af-400e-822f-725ce1afb0b2",
   "metadata": {},
   "source": [
    "## Choosing the \"No Emission\" Date\n",
    "\n",
    "Next we choose the no emission date using the same process. This is the dataset we will compare the \"Active Emission\" one too. The recommended choice is the satelite overpass immediately before the \"Active Emission\" one. \n",
    "\n",
    "<b>If your active emission day is 2020-01-17, it is suggested that your no emission day would be 2020-01-14. However, if background values are raised, this may indicate that another no emission day should be chosen to get a better reading</b>\n",
    "\n",
    "The one parameter you need to modify before running the code is:\n",
    "\n",
    "<p style=\"text-align: center;\"><b>temporal_extent</b> = [\"2020-01-14\", \"2020-01-14\"]</p>\n",
    "\n",
    "The temporal extent dates <b><u>MUST BE IDENTICAL</u></b>\n",
    "\n",
    "If you receive an error message of 'NoDataAvailable' then please check the list of available data above and try again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e74d95f-bc49-4233-b767-6a1daaaf68ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_temporal_extent = [\"2019-11-18\", \"2019-11-18\"] # Enter parameters for the no emission day\n",
    "\n",
    "def no_emission(site_id, temporal_extent):\n",
    "    site = studysite_csv[studysite_csv['id'] == site_id].iloc[0]\n",
    "\n",
    "    no_emission = connection.load_collection(\n",
    "        \"SENTINEL2_L2A\",\n",
    "        temporal_extent=no_temporal_extent,\n",
    "        spatial_extent={\n",
    "            \"west\": site['west'],\n",
    "            \"south\": site['south'],\n",
    "            \"east\": site['east'],\n",
    "            \"north\": site['north']\n",
    "        },\n",
    "        bands=[\"B11\", \"B12\"],\n",
    "    )\n",
    "    no_emission.download(\"Sentinel-2_no_emissionMBMP.Tiff\")\n",
    "\n",
    "no_emission(site_id, no_temporal_extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de701594-5589-41ad-8750-5a57f70cc130",
   "metadata": {},
   "source": [
    "## Downloading Background Satelite Image\n",
    "\n",
    "This section helps with locating the source of the emission by displaying a true colour satelite image of the oil/gas field that the data will be superimposed over. This will help distinguish between true emissions and visual spectrum observable clouds. It is recommended that you choose the same date as your active emission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9db215-2b88-4bf4-b65c-06dc75e94a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The truecolour raster needs to be reprojected to WGS84 line up correctly with the folium map.\n",
    "The same will be done later for the SWIR dataset.\n",
    "\"\"\"\n",
    "def reproject_to_epsg4326(data, meta):\n",
    "    target_crs = \"EPSG:4326\"\n",
    "    \n",
    "    # Calculate transform and metadata for the target CRS\n",
    "    transform, width, height = calculate_default_transform(\n",
    "        meta['crs'], target_crs, meta['width'], meta['height'], *meta['bounds']\n",
    "    )\n",
    "    \n",
    "    # Update metadata for the new projection\n",
    "    new_meta = meta.copy()\n",
    "    new_meta.update({\n",
    "        \"crs\": target_crs,\n",
    "        \"transform\": transform,\n",
    "        \"width\": width,\n",
    "        \"height\": height,\n",
    "    })\n",
    "    \n",
    "    # Prepare an array for reprojected data\n",
    "    reprojected_data = []\n",
    "    for i in range(meta['count']):\n",
    "        # Create an empty numpy array to store the reprojected data for the band\n",
    "        destination = np.empty((height, width), dtype=data[i].dtype)\n",
    "        reproject(\n",
    "            source=data[i],\n",
    "            destination=destination,\n",
    "            src_transform=meta['transform'],\n",
    "            src_crs=meta['crs'],\n",
    "            dst_transform=transform,\n",
    "            dst_crs=target_crs,\n",
    "            resampling=Resampling.nearest\n",
    "        )\n",
    "        reprojected_data.append(destination)\n",
    "    \n",
    "    return np.array(reprojected_data), new_meta  # Returning as numpy array instead of writing to file\n",
    "\n",
    "\n",
    "# The truecolour download uses the same date as the active_emission function.\n",
    "def truecolour_image(site_id, temporal_extent):\n",
    "\n",
    "    site = studysite_csv[studysite_csv['id'] == site_id].iloc[0]\n",
    "\n",
    "    truecolour_image = connection.load_collection(\n",
    "        \"SENTINEL2_L2A\",\n",
    "        temporal_extent=temporal_extent,\n",
    "        spatial_extent={\n",
    "            \"west\": site['west'],\n",
    "            \"south\": site['south'],\n",
    "            \"east\": site['east'],\n",
    "            \"north\": site['north']\n",
    "        },\n",
    "        bands=[\"B02\", \"B03\", \"B04\"],\n",
    "    )\n",
    "    \n",
    "    # Download the true colour image\n",
    "    file_path = \"Sentinel-2_truecolourMBMP.Tiff\"\n",
    "    truecolour_image.download(file_path)\n",
    "    \n",
    "    # Read the file into memory\n",
    "    with rasterio.open(file_path) as src:\n",
    "        data = [src.read(i) for i in range(1, src.count + 1)]\n",
    "        meta = src.meta.copy()\n",
    "        meta['bounds'] = src.bounds\n",
    "\n",
    "    # Reproject the data in memory\n",
    "    reprojected_data, reprojected_meta = reproject_to_epsg4326(data, meta)\n",
    "    \n",
    "    # Return reprojected data and metadata\n",
    "    return reprojected_data, reprojected_meta\n",
    "\n",
    "# Run and store the reprojected image as a variable\n",
    "temporal_extent = active_temporal_extent\n",
    "reprojected_image_data, reprojected_image_meta = truecolour_image(site_id, temporal_extent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fde51c-73cf-4816-b880-bf4715dc35aa",
   "metadata": {},
   "source": [
    "## Running Plume Visualiser Analysis\n",
    "The code below will use the satelite data to display plumes above 2000kg/h in ideal conditions. Provided all the variables above have been run correctly, this next section should take moments to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418e39a4-71fd-4190-9838-2348bd4ee736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to align the datasets on the folium map\n",
    "def get_bounds(site_id, csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    site = df[df['id'] == site_id]\n",
    "    if site.empty:\n",
    "        raise ValueError(f\"Site ID {site_id} not found in the CSV file.\")\n",
    "    site = site.iloc[0]\n",
    "    return [[site['south'], site['west']], [site['north'], site['east']]]\n",
    "\n",
    "csv_path = r'C:\\GIS_Course\\Methane_Point_Detection\\Sentinel-2_Algeria_Methane\\Data\\Algerian_Oil_and_Gas_Fields.csv'\n",
    "bounds = get_bounds(site_id, csv_path)\n",
    "\n",
    "# File path definitions\n",
    "Active_Multiband = \"Sentinel-2_active_emissionMBMP.Tiff\"\n",
    "No_Multiband = \"Sentinel-2_no_emissionMBMP.Tiff\"\n",
    "output_file = \"SWIR_diff.tiff\"\n",
    "masked_output_file = \"SWIR_diff_masked_urban.tiff\"\n",
    "urban_geojson = r\"C:\\GIS_Course\\Methane_Point_Detection\\Sentinel-2_Algeria_Methane\\hassi_messaoud_urban.geojson\"\n",
    "\n",
    "# The main MBMP calculations begin here \n",
    "with rasterio.open(Active_Multiband) as Active_img, rasterio.open(No_Multiband) as No_img:\n",
    "    \n",
    "    # These divisions convert the Sentinel-2 L1C digital numbers to reflectance data.\n",
    "    Active_B11 = Active_img.read(1).astype(float) / 10000.0 \n",
    "    Active_B12 = Active_img.read(2).astype(float) / 10000.0\n",
    "    No_B11 = No_img.read(1).astype(float) / 10000.0\n",
    "    No_B12 = No_img.read(2).astype(float) / 10000.0\n",
    "\n",
    "    #This perfoms two MBSP calculations, one for each satelite pass.\n",
    "    MBSP_active = (Active_B12 - Active_B11) / Active_B11\n",
    "    MBSP_no = (No_B12 - No_B11) / No_B11\n",
    "\n",
    "    #This perfoms the MBMP calculation.\n",
    "    SWIR_diff = MBSP_active - MBSP_no\n",
    "\n",
    "# Reproject and save SWIR_diff to EPSG:4326 for the folium map\n",
    "with rasterio.open(Active_Multiband) as src:\n",
    "    target_crs = \"EPSG:4326\"\n",
    "    transform, width, height = calculate_default_transform(\n",
    "        src.crs, target_crs, src.width, src.height, *src.bounds\n",
    "    )\n",
    "    meta = src.meta.copy()\n",
    "    meta.update({\n",
    "        \"crs\": target_crs,\n",
    "        \"transform\": transform,\n",
    "        \"width\": width,\n",
    "        \"height\": height,\n",
    "        \"count\": 1,\n",
    "        \"dtype\": SWIR_diff.dtype\n",
    "    })\n",
    "    with rasterio.open(output_file, \"w\", **meta) as dest:\n",
    "        reproject(\n",
    "            source=SWIR_diff,\n",
    "            destination=rasterio.band(dest, 1),\n",
    "            src_transform=src.transform,\n",
    "            src_crs=src.crs,\n",
    "            dst_transform=transform,\n",
    "            dst_crs=target_crs,\n",
    "            resampling=Resampling.nearest\n",
    "        )\n",
    "\"\"\"\n",
    "Urban areas proved to be a problem whne segmenting the plume from the scene. \n",
    "These have been masked but this is an imperfect solution as plume length is \n",
    "a strong predictor of emission rate and this can be cut off by the mask. \n",
    "It should be assumed that plumes that cross urban areas are underestimated.\n",
    "\"\"\"\n",
    "# Load GeoJSON and create urban mask\n",
    "urban_areas = gpd.read_file(urban_geojson)\n",
    "with rasterio.open(output_file) as src:\n",
    "    urban_areas = urban_areas.to_crs(src.crs)\n",
    "\n",
    "    # Rasterize the urban areas\n",
    "    urban_mask = geometry_mask(\n",
    "        [feature[\"geometry\"] for feature in urban_areas.to_crs(src.crs).__geo_interface__[\"features\"]],\n",
    "        out_shape=(src.height, src.width),\n",
    "        transform=src.transform,\n",
    "        invert=True\n",
    "    )\n",
    "\n",
    "    # Apply the urban area mask to SWIR_diff\n",
    "    swir_diff = src.read(1)\n",
    "    swir_diff_masked = np.where((urban_mask) | (swir_diff == -0.0), -32768, -swir_diff)\n",
    "    swir_diff_masked = np.where(swir_diff_masked > 3000, -32768, swir_diff_masked)\n",
    "    \"\"\" \n",
    "    to account for seasonality, the median value of the dataset (i.e. the background) has been \n",
    "    adjusted to zero\n",
    "    \"\"\"\n",
    "    \n",
    "    target_median = 0.0  # Define target median value\n",
    "\n",
    "    # Compute the current median (ignoring NoData values)\n",
    "    current_median = np.median(swir_diff_masked[(swir_diff_masked > -3000) & (swir_diff_masked < 3000)])\n",
    "\n",
    "    # Compute shift needed\n",
    "    shift_value = target_median - current_median\n",
    "\n",
    "    # Apply shift to all valid pixels\n",
    "    swir_diff_masked = np.where(\n",
    "        (swir_diff_masked > -3000) & (swir_diff_masked < 3000),\n",
    "        swir_diff_masked + shift_value,\n",
    "        -32768\n",
    "    )\n",
    "\n",
    "    print(f\"Adjusting median from {current_median} to {target_median}, shifting by {shift_value}\")\n",
    "    \n",
    "    # Save the masked SWIR_diff to a new file\n",
    "    meta = src.meta.copy()\n",
    "    meta.update(dtype=rasterio.float32, nodata=np.nan)\n",
    "    with rasterio.open(masked_output_file, \"w\", **meta) as dest:\n",
    "        dest.write(swir_diff_masked.astype(rasterio.float32), 1)\n",
    "\n",
    "# Calculate centre for map using masked SWIR_diff raster bounds\n",
    "with rasterio.open(masked_output_file) as src:\n",
    "    map_bounds = src.bounds\n",
    "    centre_lat = (map_bounds.top + map_bounds.bottom) / 2\n",
    "    centre_lon = (map_bounds.left + map_bounds.right) / 2\n",
    "\n",
    "# Create Folium map\n",
    "m = Map(location=[centre_lat, centre_lon], zoom_start=10, control_scale=True)\n",
    "\n",
    "# Use the reprojected image stored in memory instead of loading from a file\n",
    "blue, green, red = reprojected_image_data[0], reprojected_image_data[1], reprojected_image_data[2]\n",
    "\n",
    "# this is to adjust the brightness of the truecolour image as it can be a little dark sometimes.\n",
    "brightness_factor = 0.03 # only change this number\n",
    "blue = np.clip(blue * brightness_factor, 0, 255)\n",
    "green = np.clip(green * brightness_factor, 0, 255)\n",
    "red = np.clip(red * brightness_factor, 0, 255)\n",
    "\n",
    "# Stack bands to create RGB image\n",
    "rgb = np.dstack((red, green, blue))\n",
    "rgb = rgb / rgb.max()\n",
    "rgb = np.log1p(rgb)\n",
    "rgb = rgb / rgb.max()\n",
    "\n",
    "# Add true colour image overlay\n",
    "with rasterio.open(masked_output_file) as src:\n",
    "    swir_bounds = [[src.bounds.bottom, src.bounds.left], [src.bounds.top, src.bounds.right]]\n",
    "\n",
    "truecolour_overlay = ImageOverlay(\n",
    "    name=\"Truecolour\",\n",
    "    image=rgb,\n",
    "    bounds=swir_bounds,\n",
    "    opacity=1,  # Lower opacity for blending with SWIR overlay\n",
    "    interactive=True,\n",
    "    zindex=1,  # Lower zindex to place below SWIR overlay\n",
    ")\n",
    "truecolour_overlay.add_to(m)\n",
    "\n",
    "# Load and stretch SWIR_diff for visualization\n",
    "with rasterio.open(masked_output_file) as src:\n",
    "    swir_bounds = [[src.bounds.bottom, src.bounds.left], [src.bounds.top, src.bounds.right]]\n",
    "    swir_data = src.read(1)\n",
    "\n",
    "    # Mask invalid data and clip negative values\n",
    "    swir_data = np.ma.masked_invalid(swir_data)\n",
    "    swir_data = np.ma.masked_where((swir_data <= -3000) | (swir_data >= 3000), swir_data)\n",
    "\n",
    "    # Calculate mean and std only for valid data\n",
    "    filtered_swir_data = swir_data[swir_data >= -3000]  # Ignore values below 3000\n",
    "    mean = np.nanmean(filtered_swir_data)\n",
    "    std = np.nanstd(filtered_swir_data)\n",
    "    std_factor = 2  # Stretch factor\n",
    "\n",
    "    # Calculate stretching bounds within the valid data range\n",
    "    lower_bound = max(mean - std_factor * std, swir_data.min())\n",
    "    upper_bound = min(mean + std_factor * std, swir_data.max())\n",
    "\n",
    "    # Normalize the data to [0, 1]\n",
    "    normalized_swir_data = np.clip((swir_data - lower_bound) / (upper_bound - lower_bound), 0, 1)\n",
    "\n",
    "    # Apply colourmap\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    rgb_data = (cmap(normalized_swir_data.filled(0))[:, :, :3] * 255).astype(np.uint8)\n",
    "\n",
    "# Add SWIR_diff overlay to map\n",
    "swir_overlay = ImageOverlay(\n",
    "    name=\"SWIR Data\",\n",
    "    image=rgb_data,\n",
    "    bounds=swir_bounds,\n",
    "    opacity=1,  # Adjust opacity for visibility\n",
    "    interactive=True,\n",
    "    zindex=2  # Ensure SWIR overlay is above other layers\n",
    ")\n",
    "swir_overlay.add_to(m)\n",
    "\n",
    "\"\"\" \n",
    "The next section adds known plume locations as red boxes around the sites. \n",
    "\"\"\"\n",
    "# Add GeoJSON data as a layer group\n",
    "vector_point_path = r\"C:\\GIS_Course\\Methane_Point_Detection\\Sentinel-2_Algeria_Methane\\Data\\known_point_sources.geojson\"\n",
    "gdf = gpd.read_file(vector_point_path)\n",
    "geojson_layer = FeatureGroup(name=\"Known Point Sources\", show=True)\n",
    "for _, row in gdf.iterrows():\n",
    "    lat, lon = row.geometry.y, row.geometry.x\n",
    "    box_size = 0.002  # Approximate size for 20x20 pixels (adjust if needed)\n",
    "    bounds = [[lat - box_size, lon - box_size], [lat + box_size, lon + box_size]]\n",
    "    rect = Rectangle(\n",
    "        bounds=bounds,\n",
    "        color=\"red\",\n",
    "        fill=False,\n",
    "    )\n",
    "    geojson_layer.add_child(rect)\n",
    "geojson_layer.add_to(m)\n",
    "\n",
    "\"\"\" \n",
    "This creates a clickable lat long popup event on the \n",
    "map that will be used for tagging the plumes\n",
    "\"\"\"\n",
    "LayerControl().add_to(m)\n",
    "m.add_child(LatLngPopup())\n",
    "\n",
    "# Display map\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8623c7-0247-4561-b42d-3e5922b60225",
   "metadata": {},
   "source": [
    "## Plume tagging\n",
    "\n",
    "Over an area the size of an oil and gas field, many objects can erroneously show up as methane like signals if a method like thresholding were used. These include urban areas, agricultural irrigation projects and new constructions. \n",
    "\n",
    "To deal with this problem, 10 known plume locations have been programmed into the system which will automatically detect plumes at those locations (Marked as red squares on the map above). Should a plume be located away from these predefined areas, a manual tagging system can be used using the guide below.\n",
    "\n",
    "![Plume Identification](Data/Plume_Identification.jpg)\n",
    "\n",
    "| Scenario | True-colour scene                                | MBMP/SWIR scene                                   | CH4 Plume? |\n",
    "|----------|--------------------------------------------------|--------------------------------------------------|------------|\n",
    "| A        | Plume visible                                    | No plume visible                                 | No         |\n",
    "| B        | No plume visible                                 | Bright four-pointed star like diffraction spike  | No         |\n",
    "| C        | No plume visible                                 | Plume visible with four-pointed diffraction spike| Yes        |\n",
    "| D        | No plume visible                                 | Plume visible                                    | Yes        |\n",
    "\n",
    "\n",
    "To tag a plume that is not near one of the predefined pins, click on a plume somewhere along its length, and then copy the given latitude and longitude coordinates into the code box below, using the following format:\n",
    "<p style=\"text-align: center;\"># User inpitted plumes go below this message.</p>\n",
    "\n",
    "Or if the plume is segmented into more than one piece, use the following format, adding as many coordinates as needed. \n",
    "\n",
    "<p style=\"text-align: center;\">[(31.6584, 5.9054)],  # User plume 1 (latitude, longitude)</p> \n",
    "<p style=\"text-align: center;\">[(31.6584, 5.9054), (31.6594, 5.9074)],  # User plume 2 (latitude, longitude)</p> \n",
    "\n",
    "Additional lines for more plumes can be added as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6528c93-29a2-4f84-8943-d5008410194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plume_coords = [\n",
    "    [(31.6584, 5.9054)],  # Site 1 DO NOT DELETE THESE!\n",
    "    [(31.6174, 5.9671)],  # Site 2\n",
    "    [(31.7570, 5.9423)],  # Site 3\n",
    "    [(31.7341, 5.9670)],  # Site 4\n",
    "    [(31.7678, 5.9999)],  # Site 5\n",
    "    [(31.7777, 5.9957)],  # Site 6\n",
    "    [(31.7975, 6.0109)],  # Site 7\n",
    "    [(31.7570, 6.1692)],  # Site 8\n",
    "    [(31.8054, 6.1551)],  # Site 9\n",
    "    [(31.8640, 6.1733)],  # Site 10\n",
    "\n",
    "    # User input plumes go below this message. Add more lines as needed\n",
    "    # [(31.7789, 5.9955)],  # User plume 1\n",
    "    [(31.8566, 6.2138), (31.8774, 6.2335)],  # User plume 2\n",
    "    # [(31.6597, 5.8990)],  # User plume 3 \n",
    "    # [(31.8580, 6.2153)],  # User plume 4 \n",
    "] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1567e74-955f-4dbb-b8ac-7b9c1b68671e",
   "metadata": {},
   "source": [
    "## Regression Model\n",
    "\n",
    "A regression model is a statistical tool used to predict a dependent variable (here, methane emission rate in kg/h or \"Q\") based on independent variables. It works by identifying relationships in the training data and using these to estimate outcomes for new data.\n",
    "\n",
    "To train the model, data from methane plumes with emission rates documented in peer-reviewed studies was collected (Gorroño et al., 2023; Pandey et al., 2023; Varon et al., 2021; Wang et al., 2023; Sanchez-Garcia et al., 2021). These plumes were then found using the MBMP Plume Visualiser. Each plume was measured for:\n",
    "\n",
    "- **CS Sum**: The plume intensity in its cross-section.\n",
    "- **Plume Length**: The plume's length in pixels.\n",
    "- **Wind Speed**: ERA5 reanalysis data for the wind speed at the time of observation.\n",
    "\n",
    "The regression analysis identifies how these factors relate to emission rates, allowing the model to predict methane emissions for other plumes based on their characteristics.\n",
    "\n",
    "Below are the data that was collected for the regression analysis. The data used for the model as of publication, is listed below.\n",
    "\n",
    "| Source                            | Long      | Lat       | Date       | Q (kg/h) | C/S Sum   | Wind (m/s) | Length (m) | Width (px) |\n",
    "|-----------------------------------|-----------|-----------|------------|----------|-----------|------------|------------|------------|\n",
    "| Gorroño et al., 2023              | 6.1545    | 31.8066   | 2021-08-31 | 5453     | 0.066746  | 4.45       | 16.125     | 7          |\n",
    "| Pandey et al., 2023               | 6.1736    | 31.8647   | 2020-01-04 | 21000    | 0.574297  | 3.65       | 294.544    | 47         |\n",
    "| Varon et al., 2021                | 5.9053    | 31.6585   | 2019-11-20 | 8497     | 0.682773  | 0.49       | 106.231    | 38         |\n",
    "| Sanchez-Garcia et al., 2021       | 6.0015    | 31.769    | 2021-08-19 | 4326     | 0.137231  | 0.96       | 43.174     | 13         |\n",
    "| Sanchez-Garcia et al., 2021       | 5.9952    | 31.7789   | 2021-08-19 | 2160     | 0.095425  | 0.96       | 13.601     | 11         |\n",
    "| Sanchez-Garcia et al., 2021       | 6.0107    | 31.7981   | 2021-08-19 | 2757     | 0.100792  | 0.96       | 16.124     | 8          |\n",
    "| Radman et al. 2023                | 5.9055    | 31.659    | 2020-01-07 | 8240     | 0.692199  | 1.44       | 164.125    | 64         |\n",
    "| Carbon Mapper Website             | 5.9954    | 31.7775   | 2023-01-31 | 3400     | 0.124114  | 2.3        | 18.788     | 8          |\n",
    "| Carbon Mapper Website             | 5.9934    | 31.7772   | 2024-09-29 | 3000     | 0.071961  | 8.88       | 20.125     | 7          |\n",
    "| Naus et al. 2023                  | 6.1684    | 31.7571   | 2020-01-14 | 3700     | 0.332890  | 1.92       | 53.460     | 30         |\n",
    "| Naus et al. 2023                  | 5.9674    | 31.6172   | 2020-01-02 | 3600     | 0.206687  | 1.33       | 38.013     | 13         |\n",
    "| Naus et al. 2023                  | 5.9917    | 31.7776   | 2020-08-06 | 4800     | 0.070314  | 5.66       | 18.439     | 8          |\n",
    "| Naus et al. 2023                  | 5.9987    | 31.7692   | 2020-08-14 | 3400     | 0.100685  | 5.13       | 21.540     | 13         |\n",
    "| Naus et al. 2023                  | 5.9677    | 31.7341   | 2020-02-28 | 2700     | 0.275828  | 0.22       | 60.745     | 30         |\n",
    "| Naus et al. 2023                  | 5.9422    | 31.7569   | 2020-02-28 | 2100     | 0.067852  | 0.22       | 8.485      | 6          |\n",
    "| Naus et al. 2023                  | 5.8986    | 31.66     | 2020-07-30 | 14800    | 0.430201  | 5.51       | 72.173     | 35         |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa51b62b-51aa-438d-b622-a7c68dcd6c90",
   "metadata": {},
   "source": [
    "## How to improve this model\n",
    "\n",
    "Below more example plumes can be added to improve the model, should more studies become available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eff60e-16cc-4173-817f-8e9d758da69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_data = pd.read_csv(r\"C:\\GIS_Course\\Methane_Point_Detection\\Sentinel-2_Algeria_Methane\\Data\\model_training_data.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf9ea84-33c0-40b7-a932-2709086f888a",
   "metadata": {},
   "source": [
    "## Detemining wind speed\n",
    "\n",
    "Wind speed is a crucial factor in determining emission rate. This next code box determines the wind speed 10m above the ground on the \"Active Emission\" date as part of the gas flux calculation using the Climate Data Store API. \n",
    "\n",
    "Access to the API requires some intital setup, details of which can be found in this software's accompanying how to guide. Several warning messages will appear but these can be ignored. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0feeddb-5d92-4834-b681-d64679f6a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The first function calculates a centroid for the wind speed location \n",
    "using the bounding box of the study area as the ERA5 API requires a \n",
    "point location. \n",
    "\"\"\"\n",
    "def get_location_from_site_id(site_id, csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    site = df[df['id'] == site_id]\n",
    "    site = site.iloc[0]\n",
    "    centre_lat = (site['south'] + site['north']) / 2\n",
    "    centre_lon = (site['west'] + site['east']) / 2\n",
    "    return {'latitude': centre_lat, 'longitude': centre_lon}\n",
    "\n",
    "# Get the location for the ERA5 data request\n",
    "location = get_location_from_site_id(site_id, csv_path)\n",
    "\n",
    "\"\"\"\n",
    "The cdsapi needs to be set up as per the instructions in the How\n",
    "to Guide or this will not work!\n",
    "\"\"\"\n",
    "c = cdsapi.Client()\n",
    "\n",
    "date = active_temporal_extent[0]  # this takes the date to be the same as the active_emission function. \n",
    "\n",
    "\"\"\"\n",
    "This tool is hard coded to retrieve data for 10:00am as Sentinel-2\n",
    "overpasses occur at around 10:30am If this tool is reconfigured for\n",
    "another region of the world this may need to be adjusted. \n",
    "\"\"\"\n",
    "# Retrieve ERA5 data and store it in a temporary file\n",
    "with NamedTemporaryFile(suffix='.nc') as tmp_file:\n",
    "    result = c.retrieve(\n",
    "        'reanalysis-era5-single-levels',\n",
    "        {\n",
    "            'product_type': 'reanalysis',\n",
    "            'variable': ['10m_u_component_of_wind', '10m_v_component_of_wind'],\n",
    "            'year': date.split('-')[0],\n",
    "            'month': date.split('-')[1],\n",
    "            'day': date.split('-')[2],\n",
    "            'time': ['10:00'],  # Sentinel 2 overpasses are at around 10:30 am over Algeria. \n",
    "            'format': 'netcdf',  # NetCDF format\n",
    "            'area': [\n",
    "                location['latitude'] + 0.25, location['longitude'] - 0.25,\n",
    "                location['latitude'] - 0.25, location['longitude'] + 0.25,\n",
    "            ],  \n",
    "        }\n",
    "    )\n",
    "    # Download data to the temporary file\n",
    "    result.download(tmp_file.name)\n",
    "    \n",
    "    # Load the dataset with xarray\n",
    "    ds = xr.open_dataset(tmp_file.name)\n",
    "\n",
    "\"\"\"\n",
    "ERA5 data provides wind speed in east/west (u10) and north/south (v10). \n",
    "Positive u10 and v10 equals a east and north wind. Negative values are\n",
    "the reverse.  \n",
    "\"\"\"\n",
    "# Extract u and v components\n",
    "u10 = ds['u10'].sel(latitude=location['latitude'], longitude=location['longitude'], method='nearest')\n",
    "v10 = ds['v10'].sel(latitude=location['latitude'], longitude=location['longitude'], method='nearest')\n",
    "\n",
    "\"\"\" \n",
    "u10 and v10 form two sides of a right angled triangle so we can calculate\n",
    "the wind speed using the A^2 + B^2 = C^2 (Pythagoras, 530 BCE). With the \n",
    "wind variables this would be: u10^2 + v10^2 = windspeed^2. So this can be \n",
    "reconfigured as wind speed = the squareroot of (u10² + v10²).\n",
    "\"\"\"\n",
    "# Wind speed calculation\n",
    "wind_speed = np.sqrt(u10**2 + v10**2)\n",
    "\n",
    "# Extract wind speed value\n",
    "wind_speed_value = wind_speed.values.item() \n",
    "print(f\"Wind Speed at 10:00 on {date}: {wind_speed_value:.2f} m/s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3930168-7e93-4e4c-9f83-011f6f6ea8c7",
   "metadata": {},
   "source": [
    "## Running the tagged plume analysis\n",
    "\n",
    "The next code box analyses methane plumes we tagged earlier and provides the following information:\n",
    "\n",
    "- **Plume Insights**: Locations, sizes, and predicted methane emission rates (kg/h) visualised on an interactive map and summarised in a table.\n",
    "- **Model Evaluation**: Details on the regression model used to estimate emissions, including its performance metrics.\n",
    "\n",
    "There are potentially to variables that can be anjusted should the model not identify plumes adequately. Firstly \"window size\" which is the size of the search box around the coordinate you selected. If a bright non-plume object is being picked up instead of the plume you clicked this can be reduced. \n",
    "\n",
    "        # This defines a 50 pixel by 50 pixel search box (25 pixels in each direction from the centre)\n",
    "        window_size = 25  <---- change this if needed  \n",
    "\n",
    "Secondly, for a plume to be counted as a plume a sufficent number of high value pixels need to be adjacent to one another. A cluster size of 60 may mean very small plumes are not picked up by the code. You can reduce the \"min_cluster_size\" to deal with this but the system will misidentify more features. \n",
    "\n",
    "        # Set a minimum threshold for a valid plume detection\n",
    "        min_cluster_size = 60  <---- change this if needed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8ddc9d-9dfa-4b93-a935-df53d8ceb5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "swir_diff_path = r'C:\\GIS_Course\\Methane_Point_Detection\\Sentinel-2_Algeria_Methane\\SWIR_diff_masked_urban.tiff'\n",
    "\n",
    "# Open the swir_diff_path file for analysis.\n",
    "with rasterio.open(swir_diff_path) as tiff_file:\n",
    "    bounds = tiff_file.bounds\n",
    "    raster_data = tiff_file.read(1)  # Read the first band\n",
    "    transform = tiff_file.transform\n",
    "\n",
    "    # Define nodata_value\n",
    "    nodata_value = tiff_file.nodata  # Extract from metadata if available\n",
    "    if nodata_value is None:\n",
    "        nodata_value = -32768  \n",
    "\n",
    "# Converts raster data into a masked array and hides the no data pixels.\n",
    "masked_data = np.ma.masked_equal(raster_data, nodata_value)\n",
    "\n",
    "# Compute min and max from remaining pixels.\n",
    "lower_bound = masked_data.min()  \n",
    "upper_bound = masked_data.max()  \n",
    "normalized_data = (masked_data - lower_bound) / (upper_bound - lower_bound)\n",
    "\n",
    "\"\"\" calculate_plume_length determines:\n",
    "    - plume_length: The longest distance between any two points in the plume (float).\n",
    "    \"\"\"\n",
    "def calculate_plume_length(plume_pixels):\n",
    "    # Compute the convex hull.\n",
    "    hull = ConvexHull(plume_pixels)\n",
    "    hull_points = plume_pixels[hull.vertices]\n",
    "    \n",
    "    # Compute plume length as the maximum pairwise distance between hull points.\n",
    "    plume_length = pdist(hull_points).max()\n",
    "    return plume_length\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Using the plume_coords defined earlier, the next function:\n",
    "      - Uses a 50×50 search box around each merge point in case the user doesn't quite click the right position,\n",
    "      - Identifies connected plume pixels within that box and requires that the largest\n",
    "        connected group has at least min_cluster_size pixels. This is done to deal with noise being classified as a plume.\n",
    "      - Extracts the plume pixels corresponding to the most common plume label,\n",
    "      - If a plume has more than one coordinate tag (the plume is segmented) then it merges them together,\n",
    "      - And returns a merged convex hull around the plume.\n",
    "    \"\"\"\n",
    "def merge_plume_segments(labeled_array, merge_coords, transform, min_cluster_size=50):\n",
    "    window_size = 25 # This defines a 50 pixel by 50 pixel search box (25 pixels in each direction from the centre)\n",
    "    merged_pixels_list = [] # empty container for the plume pixels.\n",
    "    \n",
    "    for lat, lon in merge_coords:\n",
    "        # Convert lat/lon to pixel coordinates.\n",
    "        row, col = rasterio.transform.rowcol(transform, lon, lat)\n",
    "        row, col = int(row), int(col)\n",
    "        \n",
    "        # Define the search box boundaries.\n",
    "        row_start = max(0, row - window_size)\n",
    "        row_end = min(labeled_array.shape[0], row + window_size + 1)\n",
    "        col_start = max(0, col - window_size)\n",
    "        col_end = min(labeled_array.shape[1], col + window_size + 1)\n",
    "        \n",
    "        # Extract the local window.\n",
    "        local_window = labeled_array[row_start:row_end, col_start:col_end]\n",
    "        \n",
    "        # Create a binary mask (1 for plume, 0 for background).\n",
    "        binary_window = (local_window > 0).astype(int)\n",
    "        \n",
    "        # Label connected components in the local window.\n",
    "        labeled_clusters, _ = label(binary_window)\n",
    "        \n",
    "        # Get the sizes of the clusters (ignore background: label 0).\n",
    "        if labeled_clusters.size > 0:\n",
    "            cluster_sizes = np.bincount(labeled_clusters.ravel())[1:]\n",
    "        else:\n",
    "            cluster_sizes = []\n",
    "        \n",
    "        # Check if the largest cluster meets the minimum size.\n",
    "        if len(cluster_sizes) == 0 or cluster_sizes.max() < min_cluster_size:\n",
    "            continue\n",
    "\n",
    "        # If a valid cluster exists, choose the plume label that appears most in the local window.\n",
    "        local_labels, counts = np.unique(local_window[local_window > 0], return_counts=True)\n",
    "        if len(local_labels) == 0:\n",
    "            continue\n",
    "        plume_label = local_labels[np.argmax(counts)]\n",
    "        \n",
    "        # Extract all pixel coordinates for this plume label from the entire image.\n",
    "        plume_pixels = np.column_stack(np.where(labeled_array == plume_label))\n",
    "        merged_pixels_list.append(plume_pixels)\n",
    "    \n",
    "    if not merged_pixels_list:\n",
    "        return None, None\n",
    "    \n",
    "    # Combine pixels from all merge points.\n",
    "    merged_pixels = np.unique(np.vstack(merged_pixels_list), axis=0)\n",
    "    \n",
    "    # Compute the convex hull for the merged pixels.\n",
    "    hull = ConvexHull(merged_pixels)\n",
    "    hull_points = merged_pixels[hull.vertices]\n",
    "    merged_polygon = Polygon([(pt[1], pt[0]) for pt in hull_points])\n",
    "    \n",
    "    return merged_polygon, merged_pixels\n",
    "\n",
    "\"\"\" analyze_plume performs the main plume analysis. For each\n",
    "tagged plume it will return its emission rate. It also creates the folium map and a \n",
    "dataframe to show the data\"\"\"\n",
    "\n",
    "def analyze_plume(masked_data, plume_coords, transform, initial_centre):\n",
    "    # Create a folium map centered on the provided coordinates.\n",
    "    noradtran_map = folium.Map(location=initial_centre, zoom_start=11, control_scale=True)\n",
    "    \n",
    "    plume_results = []  # Container for plume analysis results.\n",
    "    \n",
    "    # Identify plume regions using an absolute SWIR threshold.\n",
    "    absolute_threshold = 0.009  # Pixels at or above this value are marked as plumes.\n",
    "    labeled_array, _ = label(masked_data > absolute_threshold)\n",
    "\n",
    "    # Loop through each suspected plume record (each provided as merge points).\n",
    "    for i, merge_points in enumerate(plume_coords):\n",
    "        try:\n",
    "            # Use the first merge point as the default location.\n",
    "            default_lat, default_lon = merge_points[0]\n",
    "            row, col = rasterio.transform.rowcol(transform, default_lon, default_lat)\n",
    "            row, col = int(row), int(col)\n",
    "            \n",
    "            # Merge plume segments using all provided merge points.\n",
    "            merged_polygon, merged_pixels = merge_plume_segments(labeled_array, merge_points, transform)\n",
    "            if merged_polygon is None:\n",
    "                plume_results.append({\n",
    "                    \"Plume\": i + 1,\n",
    "                    \"Location\": (default_lat, default_lon),\n",
    "                    \"Status\": \"No plume\"\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # Calculate plume length.\n",
    "            plume_length = calculate_plume_length(merged_pixels)\n",
    "            \n",
    "            # Compute the centroid of the merged plume (if needed for mapping).\n",
    "            centroid = merged_pixels.mean(axis=0)\n",
    "            \n",
    "            # Convert the merged polygon's pixel coordinates back to lat/lon for mapping.\n",
    "            merged_polygon_latlon = []\n",
    "            for x, y in merged_polygon.exterior.coords:\n",
    "                lon_map, lat_map = rasterio.transform.xy(transform, int(y), int(x))\n",
    "                merged_polygon_latlon.append((lat_map, lon_map))\n",
    "            \n",
    "            # Add the merged plume polygon to the map.\n",
    "            folium.Polygon(\n",
    "                locations=merged_polygon_latlon,\n",
    "                color=\"red\",\n",
    "                weight=3,\n",
    "                fill=False,\n",
    "                popup=f\"Merged Plume {i + 1}\"\n",
    "            ).add_to(noradtran_map)\n",
    "            \n",
    "            # Compute the total sum of all plume pixel values in the merged region.\n",
    "            plume_total_sum = np.sum(np.abs(masked_data[merged_pixels[:, 0], merged_pixels[:, 1]]))\n",
    "\n",
    "            # Append the plume result without any width or cross-sectional values.\n",
    "            plume_results.append({\n",
    "                \"Plume\": i + 1,\n",
    "                \"Location\": (default_lat, default_lon),\n",
    "                \"Length (px)\": plume_length,\n",
    "                \"Total Sum\": plume_total_sum,\n",
    "                \"plume_pixels\": merged_pixels.tolist(),\n",
    "                \"Status\": \"Detected\",\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            plume_results.append({\n",
    "                \"Plume\": i + 1,\n",
    "                \"Location\": (default_lat, default_lon),\n",
    "                \"Status\": f\"Error: {e}\"\n",
    "            })\n",
    "    \n",
    "    # Return the plume statistics and the folium map.\n",
    "    return plume_results, noradtran_map\n",
    "\n",
    "\"\"\" add_swir_data_to_map loads, normalizes, and adds a Short-Wave \n",
    "Infrared (SWIR) raster layer to the Folium map.\"\"\"\n",
    "def add_swir_data_to_map(map_object, tiff_path):\n",
    "    with rasterio.open(tiff_path) as tiff_file: \n",
    "        swir_data = tiff_file.read(1)\n",
    "        nodata_value = -32768.0  # NaN value to be ignored. \n",
    "\n",
    "    # Mask NaN pixels and normalise data.\n",
    "    masked_data = np.ma.masked_equal(swir_data, nodata_value)\n",
    "    filtered_masked_data = masked_data[masked_data >= -3000]  # Ignore values below -3000\n",
    "    mean, std = np.nanmean(filtered_masked_data), np.nanstd(filtered_masked_data)\n",
    "    lower_bound, upper_bound = mean - std_factor * std, mean + std_factor * std\n",
    "    normalized_data = (masked_data - lower_bound) / (upper_bound - lower_bound)\n",
    "    normalized_data = np.clip(normalized_data, 0, 1)\n",
    "\n",
    "    # Convert SWIR data to RGB for mapping.\n",
    "    cmap = plt.get_cmap(\"viridis\")\n",
    "    swir_rgb = (cmap(normalized_data)[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_bounds = [[bounds.bottom, bounds.left], [bounds.top, bounds.right]]\n",
    "\n",
    "    # Add SWIR overlay to the map.\n",
    "    swir_overlay = ImageOverlay(\n",
    "        name=\"SWIR Data\",\n",
    "        image=swir_rgb,\n",
    "        bounds=image_bounds,\n",
    "        opacity=1,  # Match Truecolour opacity\n",
    "        interactive=True, \n",
    "        zindex=2  # Place above Truecolour\n",
    "    )\n",
    "    swir_overlay.add_to(map_object)\n",
    "\n",
    "# Convert the dataset into a DataFrame\n",
    "model_df = pd.DataFrame(initial_data)\n",
    "\n",
    "\"\"\" Update_model trains and evaluates a XGBoost regression model \n",
    "to predict methane emission rates based on the known plume data shown\n",
    "earlier.\"\"\"\n",
    "\n",
    "def update_model(df):\n",
    "    # feature engineering \n",
    "    df[\"Wind_PL_ratio\"] = df[\"Wind\"] / df[\"PL\"]\n",
    "    df[\"PPS_times_Wind_PL\"] = df[\"PPS\"] * df[\"Wind_PL_ratio\"]\n",
    "    df[\"PPS_adjusted\"] = df[\"PPS\"] / np.cos(np.radians(df[\"SZA\"])) \n",
    "\n",
    "    # Model variables\n",
    "    X = df[[\"PPS\", \"PPS_adjusted\", \"SZA\", \"Wind_PL_ratio\", \"PPS_times_Wind_PL\"]] \n",
    "    y = df[\"IME Q\"]\n",
    "    # Apply log transformation to the target\n",
    "    y_log = np.log(y + 1)\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Optuna optimised hyperparameters\n",
    "    xgb_model = XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        n_estimators=190, # Number of trees\n",
    "        learning_rate=0.028616969156350647, # learning rate\n",
    "        max_depth=12, # how complex can a tree get?\n",
    "        min_child_weight=9, \n",
    "        subsample=0.5670170953760089,\n",
    "        colsample_bytree=0.9978969697440409,\n",
    "        gamma=0.00012313063115462585,\n",
    "        reg_alpha=0.11269249745066312,\n",
    "        reg_lambda=1.062741418622072,\n",
    "        random_state=42 # What is the meaning of life, the universe and everything?\n",
    "    )\n",
    "    xgb_model.fit(X_scaled, y_log)\n",
    "\n",
    "    return X_scaled, y, scaler, xgb_model  \n",
    "\n",
    "# Train the XGBoost model\n",
    "X_scaled, y, scaler, xgb_model = update_model(model_df)\n",
    "\n",
    "# Get the centre of the SWIR TIFF\n",
    "def get_raster_centre(tiff_path):\n",
    "    with rasterio.open(tiff_path) as tiff_file:\n",
    "        bounds = tiff_file.bounds\n",
    "        centre_lat = (bounds.top + bounds.bottom) / 2\n",
    "        centre_lon = (bounds.left + bounds.right) / 2\n",
    "    return centre_lat, centre_lon\n",
    "centre_coords = get_raster_centre(swir_diff_path)\n",
    "\n",
    "\n",
    "# Perform plume analysis, centreing the map on the SWIR TIFF\n",
    "plume_analysis_results, noradtran_map = analyze_plume(masked_data, plume_coords, transform, centre_coords)\n",
    "\n",
    "# Ensure these derived features are computed in model_df\n",
    "#model_df[\"PPS_adjusted\"] = model_df[\"PPS\"] / np.cos(np.radians(model_df[\"SZA\"])) \n",
    "#model_df[\"Wind_PL_ratio\"] = model_df[\"Wind\"] / model_df[\"PL\"]\n",
    "#model_df[\"PPS_times_Wind_PL\"] = model_df[\"PPS_adjusted\"] * model_df[\"Wind_PL_ratio\"]  \n",
    "\n",
    "\n",
    "# NEW METHOD: Use derived features ('PPS', 'SZA', 'Wind_PL_ratio', 'PPS_times_Wind_PL')\n",
    "# Ensure that model_df already has these derived features computed (as done in update_model)\n",
    "for plume in plume_analysis_results:\n",
    "    if plume[\"Status\"] == \"Detected\":\n",
    "        idx = plume[\"Plume\"] - 1  # Assume plume number corresponds to row index+1 in model_df\n",
    "        \n",
    "        pps_adjusted = plume[\"Total Sum\"] / np.cos(np.radians(sun_zenith_mean_value)) \n",
    "        \n",
    "        input_features = pd.DataFrame([[\n",
    "            plume[\"Total Sum\"],  \n",
    "            pps_adjusted,  #\n",
    "            sun_zenith_mean_value,  \n",
    "            wind_speed_value / plume[\"Length (px)\"],  \n",
    "            pps_adjusted * (wind_speed_value / plume[\"Length (px)\"])  \n",
    "        ]], columns=['PPS', 'PPS_adjusted', 'SZA', 'Wind_PL_ratio', 'PPS_times_Wind_PL'])\n",
    "\n",
    "        input_features_scaled = scaler.transform(input_features)\n",
    "        log_prediction = xgb_model.predict(input_features_scaled)\n",
    "\n",
    "        plume[\"Predicted Emission Rate (kg/h)\"] = np.exp(log_prediction[0]) - 1\n",
    "    else:\n",
    "        plume[\"Predicted Emission Rate (kg/h)\"] = \"\"\n",
    "\n",
    "\n",
    "# Convert updated results to a DataFrame\n",
    "plume_df = pd.DataFrame(plume_analysis_results)\n",
    "# Rename the index to \"Site X\" or \"User X\"\n",
    "plume_labels = [f\"Site {i + 1}\" if i < 10 else f\"User {i - 9}\" for i in range(len(plume_df))]\n",
    "plume_df.index = plume_labels\n",
    "\n",
    "# Rename columns as needed\n",
    "plume_df = plume_df.rename(columns={\n",
    "    \"Length (px)\": \"Length (px)\",\n",
    "    \"Total Sum\": \"Total Sum\",\n",
    "    \"Predicted Emission Rate (kg/h)\": \"Q (kg/h)\"\n",
    "})\n",
    "\n",
    "# Specify desired column order including \"Total Sum\"\n",
    "column_order = [\"Status\", \"Location\", \"Length (px)\", \"Total Sum\", \"Q (kg/h)\"]\n",
    "existing_columns = [col for col in column_order if col in plume_df.columns]\n",
    "plume_df = plume_df[existing_columns]\n",
    "\n",
    "# Hiding the dataframe keyfield as it isn't needed\n",
    "#if \"Plume\" in plume_df.columns: plume_df.drop(columns=[\"Plume\"], inplace=True)\n",
    "\n",
    "# Display updated DataFrame with predicted emission rates\n",
    "r_squared = xgb_model.score(X_scaled, np.log(model_df[\"IME Q\"] + 1))\n",
    "print(f\"Active Emission Date: {active_temporal_extent[0]}\")\n",
    "print(f\"No Emission Date: {no_temporal_extent[0]}\")\n",
    "print(f\"Wind Speed: {wind_speed_value:.2f} m/s\")\n",
    "pd.set_option(\"display.width\", 200)  #Controls dataframe width\n",
    "pd.set_option(\"display.max_columns\", None)  # \"None\" puts all a plume's results on one line\n",
    "# Replace NaN values with an empty string for readability\n",
    "plume_df = plume_df.fillna(\"\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(plume_df)\n",
    "\n",
    "# Add SWIR overlay to the map\n",
    "add_swir_data_to_map(noradtran_map, swir_diff_path)\n",
    "\n",
    "# Add a layer control to toggle map layers\n",
    "LayerControl().add_to(noradtran_map)\n",
    "\n",
    "# Display the map with updated analysis\n",
    "display(noradtran_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b74ded-f7d9-40f2-9a84-07db203bcdf5",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. **Gorroño, J., Varon, D.J., Irakulis-Loitxate, I. and Guanter, L., 2022.** Understanding the potential of Sentinel-2 for monitoring methane point emissions. *Atmospheric Measurement Techniques Discussions, 2022*, pp.1-25.\n",
    "\n",
    "2. **Pandey, S., van Nistelrooij, M., Maasakkers, J.D., Sutar, P., Houweling, S., Varon, D.J., Tol, P., Gains, D., Worden, J. and Aben, I., 2023.** Daily detection and quantification of methane leaks using Sentinel-3: a tiered satellite observation approach with Sentinel-2 and Sentinel-5p. *Remote Sensing of Environment, 296*, p.113716.\n",
    "\n",
    "3. **Pythagoras, 530 BCE.** A squared plus B squared equals C squared: The definitive guide to right angled triangles. Ancient Greece: Croton Press.\n",
    "\n",
    "4. **Naus, S., Maasakkers, J.D., Gautam, R., Omara, M., Stikker, R., Veenstra, A.K., Nathan, B., Irakulis-Loitxate, I., Guanter, L., Pandey, S. and Girard, M., 2023.** Assessing the relative importance of satellite-detected methane superemitters in quantifying total emissions for oil and gas production areas in algeria. Environmental Science & Technology, 57(48), pp.19545-19556.\n",
    "\n",
    "5. **Radman, A., Mahdianpari, M., Varon, D.J. and Mohammadimanesh, F., 2023.** S2MetNet: A novel dataset and deep learning benchmark for methane point source quantification using Sentinel-2 satellite imagery. *Remote Sensing of Environment, 295*, p.113708.\n",
    "\n",
    "6. **Varon, D.J., Jervis, D., McKeever, J., Spence, I., Gains, D. and Jacob, D.J., 2020.** High-frequency monitoring of anomalous methane point sources with multispectral Sentinel-2 satellite observations. *Atmospheric Measurement Techniques Discussions, 2020*, pp.1-21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8fe9c5-0b67-4e7a-b7b0-35521f324cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Paths to datasets\n",
    "training_data_path = r\"C:\\GIS_Course\\Methane_Point_Detection\\Sentinel-2_Algeria_Methane\\Data\\model_training_data.csv\"\n",
    "evaluation_data_path = r\"C:\\GIS_Course\\Methane_Point_Detection\\Sentinel-2_Algeria_Methane\\Data\\model_evaluation_data.csv\"\n",
    "\n",
    "# Load training data\n",
    "try:\n",
    "    train_df = pd.read_csv(training_data_path)\n",
    "    print(\"Training data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Training data file not found at {training_data_path}\")\n",
    "    exit()\n",
    "\n",
    "# Compute derived features for training data\n",
    "train_df[\"Wind_PL_ratio\"] = train_df[\"Wind\"] / train_df[\"PL\"]\n",
    "train_df[\"PPS_times_Wind_PL\"] = train_df[\"PPS\"] * train_df[\"Wind_PL_ratio\"]\n",
    "train_df[\"PPS_adjusted\"] = train_df[\"PPS\"] / np.cos(np.radians(train_df[\"SZA\"]))  # ✅ SZA-Normalized PPS\n",
    "\n",
    "# Select training features and target\n",
    "X_train = train_df[[\"PPS\", \"PPS_adjusted\", \"SZA\", \"Wind_PL_ratio\", \"PPS_times_Wind_PL\"]]\n",
    "y_train = np.log(train_df[\"IME Q\"] + 1)  # Apply log transformation\n",
    "\n",
    "# Standardize training features\n",
    "scaler = StandardScaler()\n",
    "X_scaled_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Train the XGBoost model\n",
    "xgb_model = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    n_estimators=190,\n",
    "    learning_rate=0.028616969156350647,\n",
    "    max_depth=12,\n",
    "    min_child_weight=9,\n",
    "    subsample=0.5670170953760089,\n",
    "    colsample_bytree=0.9978969697440409,\n",
    "    gamma=0.00012313063115462585,\n",
    "    reg_alpha=0.11269249745066312,\n",
    "    reg_lambda=1.062741418622072,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_scaled_train, y_train)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Load evaluation data\n",
    "try:\n",
    "    eval_df = pd.read_csv(evaluation_data_path)\n",
    "    print(\"Evaluation data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Evaluation data file not found at {evaluation_data_path}\")\n",
    "    exit()\n",
    "\n",
    "# Ensure necessary columns exist\n",
    "required_columns = {\"PPS\", \"SZA\", \"Wind\", \"PL\", \"IME Q\"}\n",
    "if not required_columns.issubset(eval_df.columns):\n",
    "    print(\"Error: Evaluation data is missing required columns.\")\n",
    "    exit()\n",
    "\n",
    "# Compute derived features for evaluation dataset\n",
    "eval_df[\"Wind_PL_ratio\"] = eval_df[\"Wind\"] / eval_df[\"PL\"]\n",
    "eval_df[\"PPS_times_Wind_PL\"] = eval_df[\"PPS\"] * eval_df[\"Wind_PL_ratio\"]\n",
    "eval_df[\"PPS_adjusted\"] = eval_df[\"PPS\"] / np.cos(np.radians(eval_df[\"SZA\"]))  \n",
    "\n",
    "# Select evaluation features\n",
    "X_eval = eval_df[[\"PPS\", \"PPS_adjusted\", \"SZA\", \"Wind_PL_ratio\", \"PPS_times_Wind_PL\"]]\n",
    "y_eval = eval_df[\"IME Q\"]  # Actual methane emission rates\n",
    "\n",
    "# Standardize using the same scaler from training\n",
    "X_scaled_eval = scaler.transform(X_eval)\n",
    "\n",
    "# Make predictions\n",
    "log_predictions = xgb_model.predict(X_scaled_eval)\n",
    "\n",
    "# Convert log predictions back to original scale\n",
    "predicted_emission_rates = np.exp(log_predictions) - 1\n",
    "\n",
    "# Compute RMSE and R²\n",
    "rmse = np.sqrt(mean_squared_error(y_eval, predicted_emission_rates))\n",
    "r2 = r2_score(y_eval, predicted_emission_rates)\n",
    "\n",
    "# Add predictions to DataFrame\n",
    "eval_df[\"Predicted IME Q\"] = predicted_emission_rates\n",
    "\n",
    "# Compute Error Percentage\n",
    "eval_df[\"Error %\"] = ((eval_df[\"Predicted IME Q\"] - eval_df[\"IME Q\"]) / eval_df[\"IME Q\"]) * 100\n",
    "\n",
    "# Save results to CSV\n",
    "output_csv_path = r\"C:\\GIS_Course\\Methane_Point_Detection\\Sentinel-2_Algeria_Methane\\Data\\model_evaluation_results.csv\"\n",
    "eval_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"Evaluation results saved to: {output_csv_path}\")\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(\"\\nModel Evaluation Metrics:\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Print Results Table\n",
    "print(\"\\nModel Prediction Results:\")\n",
    "results_table = eval_df[[\"IME Q\", \"Predicted IME Q\", \"Error %\"]]\n",
    "results_table.index = [f\"Plume {i+1}\" for i in range(len(results_table))]  # Label rows\n",
    "print(results_table.to_string(index=True, float_format=\"%.2f\"))\n",
    "\n",
    "# Scatter plot of Predicted vs. Actual Emissions\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_eval, predicted_emission_rates, alpha=0.6, edgecolors='k', label=\"Predictions\")\n",
    "plt.plot([y_eval.min(), y_eval.max()], [y_eval.min(), y_eval.max()], color='red', linestyle='--', linewidth=2, label=\"Ideal Fit\")\n",
    "plt.xlabel(\"Actual IME Q (kg/h)\")\n",
    "plt.ylabel(\"Predicted IME Q (kg/h)\")\n",
    "plt.title(f\"Predicted vs. Actual Methane Emissions\\nRMSE: {rmse:.2f}, R²: {r2:.4f}\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af58bc6-1f68-4aee-9604-9dd66bd510a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
