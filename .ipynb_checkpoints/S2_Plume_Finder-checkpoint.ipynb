{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7f561c3-51d4-4476-922d-876088df12c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sentinel-2 Plume Finder Tool\n",
    "\n",
    "## Overview \n",
    "This notebook provides a comprehensive workflow for detecting and analysing methane plumes from oil and gas facilities using Sentinel-2 satellite data. It combines satellite imagery processing, wind speed analysis, and regression modelling to estimate methane emission rates accurately. Key functionalities include:\n",
    "\n",
    "1. **SWIR Analysis**: Uses Sentinel-2's Short-Wave Infrared (SWIR) bands to detect methane plumes.\n",
    "2. **Plume Detection and Tagging**: Uses user driven tagging of plume locations.\n",
    "3. **Regression-Based Emission Estimation**: Employs a XGBoost regression model to estimate methane emission rates based on plume characteristics and wind speed data.\n",
    "4. **Dynamic Model Updates**: Facilitates the addition of new training data to refine the XGBoost model for improved predictions.\n",
    "5. **Interactive Visualisation**: Creates an interactive map to visualise SWIR-derived plumes, and provides an estimate of their emission rates in kg/h.\n",
    "\n",
    "This tool is designed for researchers, policymakers, and environmental analysts aiming to quantify and monitor methane emissions efficiently.\n",
    "\n",
    "The section below imports the packages needed to run the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af9cfbc-e95b-4685-a7e2-19f2ccc7cf3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Connecting to Sentinel-2 data\n",
    "import openeo\n",
    "\n",
    "# Available Date finder imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# SWIR and Truecolour processing imports\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.features import geometry_mask\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "\n",
    "# Interactive Maps and Visualisation\n",
    "import folium  # For creating interactive maps\n",
    "from folium import Map, LayerControl, LatLngPopup, Rectangle  # Map features and interactions\n",
    "from folium.raster_layers import ImageOverlay  # Overlay raster images on maps\n",
    "from folium import FeatureGroup  # For grouping map layers\n",
    "import matplotlib.pyplot as plt  # For plotting and visualisation\n",
    "\n",
    "# Wind Speed imports\n",
    "import cdsapi \n",
    "from tempfile import NamedTemporaryFile  \n",
    "import xarray as xr\n",
    "\n",
    "# Plume analysis imports\n",
    "from scipy.ndimage import label  # For segmentation and labelling of regions\n",
    "from scipy.spatial import ConvexHull  # For calculating convex hulls of shapes\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "# Imports related to predictive model  \n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67714102-a656-4414-9e45-42c8c80460ea",
   "metadata": {},
   "source": [
    "## Connect to OpenEO\n",
    "\n",
    "The code below establishes a connection with the Copernicus openEO platform which provides a wide variety of earth observation datasets\n",
    "\n",
    "- If this does not read as 'Authorised successfully' or 'Authenticated using refresh token', then please ensure that you have completed the setup steps as outlined in section 2.3.6 of the how to guide. \n",
    "\n",
    "- If you have followed the steps in section 2.3.6 correctly and the problem persists, please look at https://dataspace.copernicus.eu/news for any information about service interruptions. \n",
    "\n",
    "- If there is no news of service problems you can raise a ticket here: https://helpcenter.dataspace.copernicus.eu/hc/en-gb/requests/new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfface4-832c-4c3b-b79c-a6886878cd83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "connection = openeo.connect(url=\"openeo.dataspace.copernicus.eu\")\n",
    "connection.authenticate_oidc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d0674c-3b9c-4ab0-aafe-0526baa82e66",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dispaly Field Names\n",
    "\n",
    "This loads the oil and gas field list. Hassi Messaoud is site 86. If you are interested in a different field, please look-up its id number. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3110af1e-a1d9-4fb8-8663-51d279eac192",
   "metadata": {},
   "outputs": [],
   "source": [
    "studysite_csv = pd.read_csv(r'C:\\GIS_Course\\Methane_Point_Detection\\Sentinel-2_Algeria_Methane\\Data\\Algerian_Oil_and_Gas_Fields.csv')\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(studysite_csv.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4115bf0d-be37-48b7-bed1-f3929708b174",
   "metadata": {},
   "source": [
    "# Site selection\n",
    "\n",
    "In the code box below, specify the field number we are interested in for analysis. \n",
    "\n",
    "<p style=\"text-align: center;\"><b>site_id</b> = 86</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f938e585-a992-4004-b7a6-c71fb7322bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_id = 86  # Specify the oil and gas field ID for the field you want to examine.\n",
    "\n",
    "# Retrieve the name of the field from the dataset\n",
    "field_name = studysite_csv[studysite_csv['id'] == site_id].iloc[0]['name']\n",
    "\n",
    "# Confirmation message\n",
    "print(f\"Site {site_id} ({field_name}) loaded correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb74f40-ba0f-4dbb-9d38-08e5b33dc11f",
   "metadata": {},
   "source": [
    "# Multi-Band Multi-Pass Analysis\n",
    "\n",
    "Varon et al. (2021) showed that methane plumes from point sources could be imaged by differencing Sentinel-2’s SWIR-1 and SWIR-2 bands. The tool runs an analysis using a  multi-band-multi-pass retrieval method: \n",
    "\n",
    "First it calculates a multi-band-single-pass calculation for both active emission and no emission dates, resulting in two datasets which are then used together for a multi-band-multi-pass method. \n",
    "The multi-band-single-pass equation is as follows: \n",
    "\n",
    "\n",
    "$$ MBSP = \\frac{B12 - B11}{B11} $$\n",
    "\n",
    "\n",
    "Where:\n",
    "- $B12$ is the Sentinel-2 SWIR band 12.\n",
    "- $B11$ is the Sentinel-2 SWIR band 11.\n",
    "\n",
    "Once active emission and no emission scenes have been calculated, the following equation is used to calculate the multi-band-multi-pass raster. \n",
    "\n",
    "$$ MBMP = ActiveMBSP − NoMBSP $$\n",
    "\n",
    "Where:\n",
    "- $ActiveMBSP$ is the multiband single pass for the active emission scene\n",
    "- $NoMBSP$ is the multiband single pass for the no emission scene.\n",
    "\n",
    "The active emission scene and no emission scene are considered in this analysis to be one satellite pass apart unless there is a large amount of interference from features such as clouds or other plumes, in which case an earlier date should be selected.\n",
    "\n",
    "A final step in this analysis that has been added to scale the MBMP dataset mean to zero and all other valid pixel values by that amount. This has been done to account for seasonal variations in solar radiation levels that may affect the measurements of this tool. \n",
    "\n",
    "To begin this process we need to determine what days have available satellite  data. \n",
    "\n",
    "# Available dates for the analysis. \n",
    "\n",
    "Sentinel 2 provides data aproximately once every 2 - 3 days, so not every date you can input is valid. The code below will tell you what dates are available to use for the oil/gas field of your choice. \n",
    "\n",
    "The one parameter you need to modify before running the code is: \n",
    "\n",
    "- <b>temporal_extent</b> = [\"2020-01-01\", \"2020-01-31\"] (change this to your chosen date range using \"YYYY-MM-DD\" format.)\n",
    "\n",
    "Once you have done this run the code and the available dates should appear below in a matter of seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feb8600-dfdd-491b-bde9-2eaa760af099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the date range you want to check for available data.\n",
    "temporal_extent = [\"2020-12-20\", \"2021-01-31\"] \n",
    "\n",
    "def get_spatial_extent(site_id):\n",
    "    site = studysite_csv[studysite_csv['id'] == site_id].iloc[0]\n",
    "    return {\n",
    "        \"west\": site['west'],\n",
    "        \"south\": site['south'],\n",
    "        \"east\": site['east'],\n",
    "        \"north\": site['north']\n",
    "    }\n",
    "\n",
    "def fetch_available_dates(site_id, temporal_extent):\n",
    "    spatial_extent = get_spatial_extent(site_id)\n",
    "    catalog_url = f\"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel2/search.json?box={spatial_extent['west']}%2C{spatial_extent['south']}%2C{spatial_extent['east']}%2C{spatial_extent['north']}&sortParam=startDate&sortOrder=ascending&page=1&maxRecords=1000&status=ONLINE&dataset=ESA-DATASET&productType=L2A&startDate={temporal_extent[0]}T00%3A00%3A00Z&completionDate={temporal_extent[1]}T00%3A00%3A00Z&cloudCover=%5B0%2C{cloud_cover}%5D\"\n",
    "    response = requests.get(catalog_url)\n",
    "    response.raise_for_status()\n",
    "    catalog = response.json()\n",
    "    dates = [date.split('T')[0] for date in map(lambda x: x['properties']['startDate'], catalog['features'])]\n",
    "    return dates\n",
    " \n",
    "cloud_cover = 5 # Specifies that only scenes with >5% cloud cover are chosen\n",
    "\n",
    "available_dates = fetch_available_dates(site_id, temporal_extent)\n",
    "print(\"Available dates:\", available_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c953b692-eb71-4c15-b3d3-95025f77bcc4",
   "metadata": {},
   "source": [
    "## Choosing the \"Active Emission\" Date\n",
    "\n",
    "A so called active emission date must be chosen from one of the available datasets. This will be the chosen day we are looking for plumes.  \n",
    "\n",
    "Like before, the one parameter you need to modify before running the code is:\n",
    "\n",
    "<p style=\"text-align: center;\"><b>temporal_extent</b> = [\"2020-01-17\", \"2020-01-17\"]</p>\n",
    "\n",
    "Change this to your chosen date range using \"YYYY-MM-DD\" format. \n",
    "\n",
    "Please note that the temporal extent dates <b><u>MUST BE IDENTICAL</u></b> because we are only choosing a single date.\n",
    "\n",
    "If you recieve an error message of 'NoDataAvailable' then please check the list of available data above and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a709165f-4542-46da-af8e-a7199df573c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_temporal_extent = [\"2019-11-20\", \"2019-11-20\"] # Enter parameters for the active emission day\n",
    "\n",
    "def active_emission(site_id, active_temporal_extent):\n",
    "    site = studysite_csv[studysite_csv['id'] == site_id].iloc[0]\n",
    "\n",
    "    active_emission = connection.load_collection(\n",
    "        \"SENTINEL2_L2A\",\n",
    "        temporal_extent=active_temporal_extent,\n",
    "        spatial_extent={\n",
    "            \"west\": site['west'],\n",
    "            \"south\": site['south'],\n",
    "            \"east\": site['east'],\n",
    "            \"north\": site['north']\n",
    "        },\n",
    "        bands=[\"B11\", \"B12\"],\n",
    "    )\n",
    "    active_emission.download(\"Sentinel-2_active_emissionMBMP.Tiff\")\n",
    "\n",
    "active_emission(site_id, active_temporal_extent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadc9eea-43af-400e-822f-725ce1afb0b2",
   "metadata": {},
   "source": [
    "## Choosing the \"No Emission\" Date\n",
    "\n",
    "Next we choose the no emission date using the same process. This is the dataset we will compare the \"Active Emission\" one too. The recommended choice is the satelite overpass immediately before the \"Active Emission\" one. \n",
    "\n",
    "<b>If your active emission day is 2020-01-17, it is suggested that your no emission day would be 2020-01-14. However, if background values are raised, this may indicate that another no emission day should be chosen to get a better reading</b>\n",
    "\n",
    "The one parameter you need to modify before running the code is:\n",
    "\n",
    "<p style=\"text-align: center;\"><b>temporal_extent</b> = [\"2020-01-14\", \"2020-01-14\"]</p>\n",
    "\n",
    "The temporal extent dates <b><u>MUST BE IDENTICAL</u></b>\n",
    "\n",
    "If you receive an error message of 'NoDataAvailable' then please check the list of available data above and try again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e74d95f-bc49-4233-b767-6a1daaaf68ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_temporal_extent = [\"2019-11-18\", \"2019-11-18\"] # Enter parameters for the no emission day\n",
    "\n",
    "def no_emission(site_id, temporal_extent):\n",
    "    site = studysite_csv[studysite_csv['id'] == site_id].iloc[0]\n",
    "\n",
    "    no_emission = connection.load_collection(\n",
    "        \"SENTINEL2_L2A\",\n",
    "        temporal_extent=no_temporal_extent,\n",
    "        spatial_extent={\n",
    "            \"west\": site['west'],\n",
    "            \"south\": site['south'],\n",
    "            \"east\": site['east'],\n",
    "            \"north\": site['north']\n",
    "        },\n",
    "        bands=[\"B11\", \"B12\"],\n",
    "    )\n",
    "    no_emission.download(\"Sentinel-2_no_emissionMBMP.Tiff\")\n",
    "\n",
    "no_emission(site_id, no_temporal_extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de701594-5589-41ad-8750-5a57f70cc130",
   "metadata": {},
   "source": [
    "## Downloading Background Satelite Image\n",
    "\n",
    "This section helps with locating the source of the emission by displaying a true colour satelite image of the oil/gas field that the data will be superimposed over. This will help distinguish between true emissions and visual spectrum observable clouds. It is recommended that you choose the same date as your active emission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9db215-2b88-4bf4-b65c-06dc75e94a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The truecolour raster needs to be reprojected to WGS84 line up correctly with the folium map.\n",
    "The same will be done later for the SWIR dataset.\n",
    "\"\"\"\n",
    "def reproject_to_epsg4326(data, meta):\n",
    "    target_crs = \"EPSG:4326\"\n",
    "    \n",
    "    # Calculate transform and metadata for the target CRS\n",
    "    transform, width, height = calculate_default_transform(\n",
    "        meta['crs'], target_crs, meta['width'], meta['height'], *meta['bounds']\n",
    "    )\n",
    "    \n",
    "    # Update metadata for the new projection\n",
    "    new_meta = meta.copy()\n",
    "    new_meta.update({\n",
    "        \"crs\": target_crs,\n",
    "        \"transform\": transform,\n",
    "        \"width\": width,\n",
    "        \"height\": height,\n",
    "    })\n",
    "    \n",
    "    # Prepare an array for reprojected data\n",
    "    reprojected_data = []\n",
    "    for i in range(meta['count']):\n",
    "        # Create an empty numpy array to store the reprojected data for the band\n",
    "        destination = np.empty((height, width), dtype=data[i].dtype)\n",
    "        reproject(\n",
    "            source=data[i],\n",
    "            destination=destination,\n",
    "            src_transform=meta['transform'],\n",
    "            src_crs=meta['crs'],\n",
    "            dst_transform=transform,\n",
    "            dst_crs=target_crs,\n",
    "            resampling=Resampling.nearest\n",
    "        )\n",
    "        reprojected_data.append(destination)\n",
    "    \n",
    "    return np.array(reprojected_data), new_meta  # Returning as numpy array instead of writing to file\n",
    "\n",
    "\n",
    "# The truecolour download uses the same date as the active_emission function.\n",
    "def truecolour_image(site_id, temporal_extent):\n",
    "\n",
    "    site = studysite_csv[studysite_csv['id'] == site_id].iloc[0]\n",
    "\n",
    "    truecolour_image = connection.load_collection(\n",
    "        \"SENTINEL2_L2A\",\n",
    "        temporal_extent=temporal_extent,\n",
    "        spatial_extent={\n",
    "            \"west\": site['west'],\n",
    "            \"south\": site['south'],\n",
    "            \"east\": site['east'],\n",
    "            \"north\": site['north']\n",
    "        },\n",
    "        bands=[\"B02\", \"B03\", \"B04\"],\n",
    "    )\n",
    "    \n",
    "    # Download the true colour image\n",
    "    file_path = \"Sentinel-2_truecolourMBMP.Tiff\"\n",
    "    truecolour_image.download(file_path)\n",
    "    \n",
    "    # Read the file into memory\n",
    "    with rasterio.open(file_path) as src:\n",
    "        data = [src.read(i) for i in range(1, src.count + 1)]\n",
    "        meta = src.meta.copy()\n",
    "        meta['bounds'] = src.bounds\n",
    "\n",
    "    # Reproject the data in memory\n",
    "    reprojected_data, reprojected_meta = reproject_to_epsg4326(data, meta)\n",
    "    \n",
    "    # Return reprojected data and metadata\n",
    "    return reprojected_data, reprojected_meta\n",
    "\n",
    "# Run and store the reprojected image as a variable\n",
    "temporal_extent = active_temporal_extent\n",
    "reprojected_image_data, reprojected_image_meta = truecolour_image(site_id, temporal_extent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fde51c-73cf-4816-b880-bf4715dc35aa",
   "metadata": {},
   "source": [
    "## Running Plume Visualiser Analysis\n",
    "The code below will use the satelite data to display plumes above 2000kg/h in ideal conditions. Provided all the variables above have been run correctly, this next section should take moments to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418e39a4-71fd-4190-9838-2348bd4ee736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to align the datasets on the folium map\n",
    "def get_bounds(site_id, csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    site = df[df['id'] == site_id]\n",
    "    if site.empty:\n",
    "        raise ValueError(f\"Site ID {site_id} not found in the CSV file.\")\n",
    "    site = site.iloc[0]\n",
    "    return [[site['south'], site['west']], [site['north'], site['east']]]\n",
    "\n",
    "csv_path = r'C:\\GIS_Course\\Methane_Point_Detection\\Sentinel-2_Algeria_Methane\\Data\\Algerian_Oil_and_Gas_Fields.csv'\n",
    "bounds = get_bounds(site_id, csv_path)\n",
    "\n",
    "# File path definitions\n",
    "Active_Multiband = \"Sentinel-2_active_emissionMBMP.Tiff\"\n",
    "No_Multiband = \"Sentinel-2_no_emissionMBMP.Tiff\"\n",
    "output_file = \"SWIR_diff.tiff\"\n",
    "masked_output_file = \"SWIR_diff_masked_urban.tiff\"\n",
    "urban_geojson = r\"C:\\GIS_Course\\Methane_Point_Detection\\Sentinel-2_Algeria_Methane\\hassi_messaoud_urban.geojson\"\n",
    "\n",
    "# The main MBMP calculations begin here \n",
    "with rasterio.open(Active_Multiband) as Active_img, rasterio.open(No_Multiband) as No_img:\n",
    "    \n",
    "    # These divisions convert the Sentinel-2 L1C digital numbers to reflectance data.\n",
    "    Active_B11 = Active_img.read(1).astype(float) / 10000.0 \n",
    "    Active_B12 = Active_img.read(2).astype(float) / 10000.0\n",
    "    No_B11 = No_img.read(1).astype(float) / 10000.0\n",
    "    No_B12 = No_img.read(2).astype(float) / 10000.0\n",
    "\n",
    "    #This perfoms two MBSP calculations, one for each satelite pass.\n",
    "    MBSP_active = (Active_B12 - Active_B11) / Active_B11\n",
    "    MBSP_no = (No_B12 - No_B11) / No_B11\n",
    "\n",
    "    #This perfoms the MBMP calculation.\n",
    "    SWIR_diff = MBSP_active - MBSP_no\n",
    "\n",
    "# Reproject and save SWIR_diff to EPSG:4326 for the folium map\n",
    "with rasterio.open(Active_Multiband) as src:\n",
    "    target_crs = \"EPSG:4326\"\n",
    "    transform, width, height = calculate_default_transform(\n",
    "        src.crs, target_crs, src.width, src.height, *src.bounds\n",
    "    )\n",
    "    meta = src.meta.copy()\n",
    "    meta.update({\n",
    "        \"crs\": target_crs,\n",
    "        \"transform\": transform,\n",
    "        \"width\": width,\n",
    "        \"height\": height,\n",
    "        \"count\": 1,\n",
    "        \"dtype\": SWIR_diff.dtype\n",
    "    })\n",
    "    with rasterio.open(output_file, \"w\", **meta) as dest:\n",
    "        reproject(\n",
    "            source=SWIR_diff,\n",
    "            destination=rasterio.band(dest, 1),\n",
    "            src_transform=src.transform,\n",
    "            src_crs=src.crs,\n",
    "            dst_transform=transform,\n",
    "            dst_crs=target_crs,\n",
    "            resampling=Resampling.nearest\n",
    "        )\n",
    "\"\"\"\n",
    "Urban areas proved to be a problem whne segmenting the plume from the scene. \n",
    "These have been masked but this is an imperfect solution as plume length is \n",
    "a strong predictor of emission rate and this can be cut off by the mask. \n",
    "It should be assumed that plumes that cross urban areas are underestimated.\n",
    "\"\"\"\n",
    "# Load GeoJSON and create urban mask\n",
    "urban_areas = gpd.read_file(urban_geojson)\n",
    "with rasterio.open(output_file) as src:\n",
    "    urban_areas = urban_areas.to_crs(src.crs)\n",
    "\n",
    "    # Rasterize the urban areas\n",
    "    urban_mask = geometry_mask(\n",
    "        [feature[\"geometry\"] for feature in urban_areas.to_crs(src.crs).__geo_interface__[\"features\"]],\n",
    "        out_shape=(src.height, src.width),\n",
    "        transform=src.transform,\n",
    "        invert=True\n",
    "    )\n",
    "\n",
    "    # Apply the urban area mask to SWIR_diff\n",
    "    swir_diff = src.read(1)\n",
    "    swir_diff_masked = np.where((urban_mask) | (swir_diff == -0.0), -32768, -swir_diff)\n",
    "    swir_diff_masked = np.where(swir_diff_masked > 3000, -32768, swir_diff_masked)\n",
    "    \"\"\" \n",
    "    to account for seasonality, the median value of the dataset (i.e. the background) has been \n",
    "    adjusted to zero\n",
    "    \"\"\"\n",
    "    \n",
    "    target_median = 0.0  # Define target median value\n",
    "\n",
    "    # Compute the current median (ignoring NoData values)\n",
    "    current_median = np.median(swir_diff_masked[(swir_diff_masked > -3000) & (swir_diff_masked < 3000)])\n",
    "\n",
    "    # Compute shift needed\n",
    "    shift_value = target_median - current_median\n",
    "\n",
    "    # Apply shift to all valid pixels\n",
    "    swir_diff_masked = np.where(\n",
    "        (swir_diff_masked > -3000) & (swir_diff_masked < 3000),\n",
    "        swir_diff_masked + shift_value,\n",
    "        -32768\n",
    "    )\n",
    "\n",
    "    print(f\"Adjusting median from {current_median} to {target_median}, shifting by {shift_value}\")\n",
    "    \n",
    "    # Save the masked SWIR_diff to a new file\n",
    "    meta = src.meta.copy()\n",
    "    meta.update(dtype=rasterio.float32, nodata=np.nan)\n",
    "    with rasterio.open(masked_output_file, \"w\", **meta) as dest:\n",
    "        dest.write(swir_diff_masked.astype(rasterio.float32), 1)\n",
    "\n",
    "# Calculate centre for map using masked SWIR_diff raster bounds\n",
    "with rasterio.open(masked_output_file) as src:\n",
    "    map_bounds = src.bounds\n",
    "    centre_lat = (map_bounds.top + map_bounds.bottom) / 2\n",
    "    centre_lon = (map_bounds.left + map_bounds.right) / 2\n",
    "\n",
    "# Create Folium map\n",
    "m = Map(location=[centre_lat, centre_lon], zoom_start=10, control_scale=True)\n",
    "\n",
    "# Use the reprojected image stored in memory instead of loading from a file\n",
    "blue, green, red = reprojected_image_data[0], reprojected_image_data[1], reprojected_image_data[2]\n",
    "\n",
    "# this is to adjust the brightness of the truecolour image as it can be a little dark sometimes.\n",
    "brightness_factor = 0.03 # only change this number\n",
    "blue = np.clip(blue * brightness_factor, 0, 255)\n",
    "green = np.clip(green * brightness_factor, 0, 255)\n",
    "red = np.clip(red * brightness_factor, 0, 255)\n",
    "\n",
    "# Stack bands to create RGB image\n",
    "rgb = np.dstack((red, green, blue))\n",
    "rgb = rgb / rgb.max()\n",
    "rgb = np.log1p(rgb)\n",
    "rgb = rgb / rgb.max()\n",
    "\n",
    "# Add true colour image overlay\n",
    "with rasterio.open(masked_output_file) as src:\n",
    "    swir_bounds = [[src.bounds.bottom, src.bounds.left], [src.bounds.top, src.bounds.right]]\n",
    "\n",
    "truecolour_overlay = ImageOverlay(\n",
    "    name=\"Truecolour\",\n",
    "    image=rgb,\n",
    "    bounds=swir_bounds,\n",
    "    opacity=1,  # Lower opacity for blending with SWIR overlay\n",
    "    interactive=True,\n",
    "    zindex=1,  # Lower zindex to place below SWIR overlay\n",
    ")\n",
    "truecolour_overlay.add_to(m)\n",
    "\n",
    "# Load and stretch SWIR_diff for visualization\n",
    "with rasterio.open(masked_output_file) as src:\n",
    "    swir_bounds = [[src.bounds.bottom, src.bounds.left], [src.bounds.top, src.bounds.right]]\n",
    "    swir_data = src.read(1)\n",
    "\n",
    "    # Mask invalid data and clip negative values\n",
    "    swir_data = np.ma.masked_invalid(swir_data)\n",
    "    swir_data = np.ma.masked_where((swir_data <= -3000) | (swir_data >= 3000), swir_data)\n",
    "\n",
    "    # Calculate mean and std only for valid data\n",
    "    filtered_swir_data = swir_data[swir_data >= -3000]  # Ignore values below 3000\n",
    "    mean = np.nanmean(filtered_swir_data)\n",
    "    std = np.nanstd(filtered_swir_data)\n",
    "    std_factor = 2  # Stretch factor\n",
    "\n",
    "    # Calculate stretching bounds within the valid data range\n",
    "    lower_bound = max(mean - std_factor * std, swir_data.min())\n",
    "    upper_bound = min(mean + std_factor * std, swir_data.max())\n",
    "\n",
    "    # Normalize the data to [0, 1]\n",
    "    normalized_swir_data = np.clip((swir_data - lower_bound) / (upper_bound - lower_bound), 0, 1)\n",
    "\n",
    "    # Apply colourmap\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    rgb_data = (cmap(normalized_swir_data.filled(0))[:, :, :3] * 255).astype(np.uint8)\n",
    "\n",
    "# Add SWIR_diff overlay to map\n",
    "swir_overlay = ImageOverlay(\n",
    "    name=\"SWIR Data\",\n",
    "    image=rgb_data,\n",
    "    bounds=swir_bounds,\n",
    "    opacity=1,  # Adjust opacity for visibility\n",
    "    interactive=True,\n",
    "    zindex=2  # Ensure SWIR overlay is above other layers\n",
    ")\n",
    "swir_overlay.add_to(m)\n",
    "\n",
    "\"\"\" \n",
    "The next section adds known plume locations as red boxes around the sites. \n",
    "\"\"\"\n",
    "# Add GeoJSON data as a layer group\n",
    "vector_point_path = r\"C:\\GIS_Course\\Methane_Point_Detection\\Sentinel-2_Algeria_Methane\\Data\\known_point_sources.geojson\"\n",
    "gdf = gpd.read_file(vector_point_path)\n",
    "geojson_layer = FeatureGroup(name=\"Known Point Sources\", show=True)\n",
    "for _, row in gdf.iterrows():\n",
    "    lat, lon = row.geometry.y, row.geometry.x\n",
    "    box_size = 0.002  # Approximate size for 20x20 pixels (adjust if needed)\n",
    "    bounds = [[lat - box_size, lon - box_size], [lat + box_size, lon + box_size]]\n",
    "    rect = Rectangle(\n",
    "        bounds=bounds,\n",
    "        color=\"red\",\n",
    "        fill=False,\n",
    "    )\n",
    "    geojson_layer.add_child(rect)\n",
    "geojson_layer.add_to(m)\n",
    "\n",
    "\"\"\" \n",
    "This creates a clickable lat long popup event on the \n",
    "map that will be used for tagging the plumes\n",
    "\"\"\"\n",
    "LayerControl().add_to(m)\n",
    "m.add_child(LatLngPopup())\n",
    "\n",
    "# Display map\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8623c7-0247-4561-b42d-3e5922b60225",
   "metadata": {},
   "source": [
    "## Plume tagging\n",
    "\n",
    "Over an area the size of an oil and gas field, many objects can erroneously show up as methane like signals if a method like thresholding were used. These include urban areas, agricultural irrigation projects and new constructions. \n",
    "\n",
    "To deal with this problem, 11 known plume locations have been programmed into the system which will automatically detect plumes at those locations (Marked as red squares on the map above). Should a plume be located away from these predefined areas, a manual tagging system can be used using the guide below.\n",
    "\n",
    "![Plume Identification](Data/Plume_Identification.jpg)\n",
    "\n",
    "| Scenario | True-colour scene                                | MBMP/SWIR scene                                   | CH4 Plume? |\n",
    "|----------|--------------------------------------------------|--------------------------------------------------|------------|\n",
    "| A        | Plume visible                                    | No plume visible                                 | No         |\n",
    "| B        | No plume visible                                 | Bright four-pointed star like diffraction spike  | No         |\n",
    "| C        | No plume visible                                 | Plume visible with four-pointed diffraction spike| Yes        |\n",
    "| D        | No plume visible                                 | Plume visible                                    | Yes        |\n",
    "\n",
    "\n",
    "To tag a plume that is not near one of the predefined pins, click on a plume somewhere along its length, and then copy the given latitude and longitude coordinates into the code box below, using the following format:\n",
    "<p style=\"text-align: center;\"># User inpitted plumes go below this message.</p>\n",
    "<p style=\"text-align: center;\">(31.6887, 5.8102),  # User plume 1 (latitude, longitude)</p> <p style=\"text-align: center;\">(31.7910, 5.8263),  # User plume 2 (latitude, longitude)</p> \n",
    "\n",
    "Additional lines for more plumes can be added as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6528c93-29a2-4f84-8943-d5008410194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plume_coords = [\n",
    "    (31.6584, 5.9054),  # Site 1 DO NOT DELETE OR MODIFY THESE!\n",
    "    (31.6174, 5.9671),  # Site 2\n",
    "    (31.7419, 5.8949),  # Site 3\n",
    "    (31.7570, 5.9423),  # Site 4\n",
    "    (31.7341, 5.9670),  # Site 5\n",
    "    (31.7678, 5.9999),  # Site 6\n",
    "    (31.7777, 5.9957),  # Site 7\n",
    "    (31.7975, 6.0109),  # Site 8\n",
    "    (31.7570, 6.1692),  # Site 9\n",
    "    (31.8054, 6.1551),  # Site 10\n",
    "    (31.8640, 6.1733),  # Site 11\n",
    "\n",
    "    # User inpitted plumes go below this message. Add more lines as needed\n",
    "    #(31.6231, 5.9601),  # User plume 1\n",
    "    #(31.7186, 5.9735),  # User plume 2\n",
    "    #(31.6597, 5.8990),  # User plume 3 \n",
    "    (31.8580, 6.2153),  # User plume 4 \n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1567e74-955f-4dbb-b8ac-7b9c1b68671e",
   "metadata": {},
   "source": [
    "## Regression Model\n",
    "\n",
    "A regression model is a statistical tool used to predict a dependent variable (here, methane emission rate in kg/h or \"Q\") based on independent variables. It works by identifying relationships in the training data and using these to estimate outcomes for new data.\n",
    "\n",
    "To train the model, data from methane plumes with emission rates documented in peer-reviewed studies was collected (Gorroño et al., 2023; Pandey et al., 2023; Varon et al., 2021; Wang et al., 2023; Sanchez-Garcia et al., 2021). These plumes were then found using the MBMP Plume Visualiser. Each plume was measured for:\n",
    "\n",
    "- **CS Sum**: The plume intensity in its cross-section.\n",
    "- **Plume Length**: The plume's length in pixels.\n",
    "- **Wind Speed**: ERA5 reanalysis data for the wind speed at the time of observation.\n",
    "\n",
    "The regression analysis identifies how these factors relate to emission rates, allowing the model to predict methane emissions for other plumes based on their characteristics.\n",
    "\n",
    "Below are the data that was collected for the regression analysis. The data used for the model as of publication, is listed below.\n",
    "\n",
    "| Source                            | Long      | Lat       | Date       | Q (kg/h) | C/S Sum   | Wind (m/s) | Length (m) | Width (px) |\n",
    "|-----------------------------------|-----------|-----------|------------|----------|-----------|------------|------------|------------|\n",
    "| Gorroño et al., 2023              | 6.1545    | 31.8066   | 2021-08-31 | 5453     | 0.066746  | 4.45       | 16.125     | 7          |\n",
    "| Pandey et al., 2023               | 6.1736    | 31.8647   | 2020-01-04 | 21000    | 0.574297  | 3.65       | 294.544    | 47         |\n",
    "| Varon et al., 2021                | 5.9053    | 31.6585   | 2019-11-20 | 8497     | 0.682773  | 0.49       | 106.231    | 38         |\n",
    "| Sanchez-Garcia et al., 2021       | 6.0015    | 31.769    | 2021-08-19 | 4326     | 0.137231  | 0.96       | 43.174     | 13         |\n",
    "| Sanchez-Garcia et al., 2021       | 5.9952    | 31.7789   | 2021-08-19 | 2160     | 0.095425  | 0.96       | 13.601     | 11         |\n",
    "| Sanchez-Garcia et al., 2021       | 6.0107    | 31.7981   | 2021-08-19 | 2757     | 0.100792  | 0.96       | 16.124     | 8          |\n",
    "| Radman et al. 2023                | 5.9055    | 31.659    | 2020-01-07 | 8240     | 0.692199  | 1.44       | 164.125    | 64         |\n",
    "| Carbon Mapper Website             | 5.9954    | 31.7775   | 2023-01-31 | 3400     | 0.124114  | 2.3        | 18.788     | 8          |\n",
    "| Carbon Mapper Website             | 5.9934    | 31.7772   | 2024-09-29 | 3000     | 0.071961  | 8.88       | 20.125     | 7          |\n",
    "| Naus et al. 2023                  | 6.1684    | 31.7571   | 2020-01-14 | 3700     | 0.332890  | 1.92       | 53.460     | 30         |\n",
    "| Naus et al. 2023                  | 5.9674    | 31.6172   | 2020-01-02 | 3600     | 0.206687  | 1.33       | 38.013     | 13         |\n",
    "| Naus et al. 2023                  | 5.9917    | 31.7776   | 2020-08-06 | 4800     | 0.070314  | 5.66       | 18.439     | 8          |\n",
    "| Naus et al. 2023                  | 5.9987    | 31.7692   | 2020-08-14 | 3400     | 0.100685  | 5.13       | 21.540     | 13         |\n",
    "| Naus et al. 2023                  | 5.9677    | 31.7341   | 2020-02-28 | 2700     | 0.275828  | 0.22       | 60.745     | 30         |\n",
    "| Naus et al. 2023                  | 5.9422    | 31.7569   | 2020-02-28 | 2100     | 0.067852  | 0.22       | 8.485      | 6          |\n",
    "| Naus et al. 2023                  | 5.8986    | 31.66     | 2020-07-30 | 14800    | 0.430201  | 5.51       | 72.173     | 35         |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa51b62b-51aa-438d-b622-a7c68dcd6c90",
   "metadata": {},
   "source": [
    "## How to improve this model\n",
    "\n",
    "Below more example plumes can be added to improve the model, should more studies become available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eff60e-16cc-4173-817f-8e9d758da69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_data = {\n",
    "    \"Emission_rate_kg_h\": [5453, 21000, 8497, 4326, 2160, 2757, 8240, 3400, 3000, \n",
    "                            3700, 3600, 4800, 3400, 2700, 2100, 14800],\n",
    "    \"Cross_sectional_Adjusted_Sum\": [0.06674600, 0.57429700, 0.68277300, 0.13723100, 0.09542500, \n",
    "                                     0.10079200, 0.69219900, 0.12411400, 0.07196100, 0.33289000, \n",
    "                                     0.20668700, 0.07031400, 0.10068500, 0.27582800, 0.06785200, \n",
    "                                     0.43020100],\n",
    "    \"Plume_length\": [16.125, 294.544, 106.231, 43.174, 13.601, 16.125, 164.125, 18.788, 20.125, \n",
    "                     53.460, 38.013, 18.439, 21.541, 60.745, 8.485, 72.173],\n",
    "    \"Width\": [7.000, 47.000, 38.000, 13.000, 11.000, 8.000, 64.000, 8.000, 7.000, \n",
    "              30.000, 13.000, 8.000, 13.000, 30.000, 6.000, 35.000],\n",
    "    \"Wind_speed\": [4.45, 3.65, 0.49, 0.96, 0.96, 0.96, 1.44, 2.3, 8.88, 1.92, 1.33, \n",
    "                   5.66, 5.13, 0.22, 0.22, 5.51]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf9ea84-33c0-40b7-a932-2709086f888a",
   "metadata": {},
   "source": [
    "## Detemining wind speed\n",
    "\n",
    "Wind speed is a crucial factor in determining emission rate. This next code box determines the wind speed 10m above the ground on the \"Active Emission\" date as part of the gas flux calculation using the Climate Data Store API. \n",
    "\n",
    "Access to the API requires some intital setup, details of which can be found in this software's accompanying how to guide. Several warning messages will appear but these can be ignored. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0feeddb-5d92-4834-b681-d64679f6a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The first function calculates a centroid for the wind speed location \n",
    "using the bounding box of the study area as the ERA5 API requires a \n",
    "point location. \n",
    "\"\"\"\n",
    "def get_location_from_site_id(site_id, csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    site = df[df['id'] == site_id]\n",
    "    site = site.iloc[0]\n",
    "    centre_lat = (site['south'] + site['north']) / 2\n",
    "    centre_lon = (site['west'] + site['east']) / 2\n",
    "    return {'latitude': centre_lat, 'longitude': centre_lon}\n",
    "\n",
    "# Get the location for the ERA5 data request\n",
    "location = get_location_from_site_id(site_id, csv_path)\n",
    "\n",
    "\"\"\"\n",
    "The cdsapi needs to be set up as per the instructions in the How\n",
    "to Guide or this will not work!\n",
    "\"\"\"\n",
    "c = cdsapi.Client()\n",
    "\n",
    "date = active_temporal_extent[0]  # this takes the date to be the same as the active_emission function. \n",
    "\n",
    "\"\"\"\n",
    "This tool is hard coded to retrieve data for 10:00am as Sentinel-2\n",
    "overpasses occur at around 10:30am If this tool is reconfigured for\n",
    "another region of the world this may need to be adjusted. \n",
    "\"\"\"\n",
    "# Retrieve ERA5 data and store it in a temporary file\n",
    "with NamedTemporaryFile(suffix='.nc') as tmp_file:\n",
    "    result = c.retrieve(\n",
    "        'reanalysis-era5-single-levels',\n",
    "        {\n",
    "            'product_type': 'reanalysis',\n",
    "            'variable': ['10m_u_component_of_wind', '10m_v_component_of_wind'],\n",
    "            'year': date.split('-')[0],\n",
    "            'month': date.split('-')[1],\n",
    "            'day': date.split('-')[2],\n",
    "            'time': ['10:00'],  # Sentinel 2 overpasses are at around 10:30 am over Algeria. \n",
    "            'format': 'netcdf',  # NetCDF format\n",
    "            'area': [\n",
    "                location['latitude'] + 0.25, location['longitude'] - 0.25,\n",
    "                location['latitude'] - 0.25, location['longitude'] + 0.25,\n",
    "            ],  \n",
    "        }\n",
    "    )\n",
    "    # Download data to the temporary file\n",
    "    result.download(tmp_file.name)\n",
    "    \n",
    "    # Load the dataset with xarray\n",
    "    ds = xr.open_dataset(tmp_file.name)\n",
    "\n",
    "\"\"\"\n",
    "ERA5 data provides wind speed in east/west (u10) and north/south (v10). \n",
    "Positive u10 and v10 equals a east and north wind. Negative values are\n",
    "the reverse.  \n",
    "\"\"\"\n",
    "# Extract u and v components\n",
    "u10 = ds['u10'].sel(latitude=location['latitude'], longitude=location['longitude'], method='nearest')\n",
    "v10 = ds['v10'].sel(latitude=location['latitude'], longitude=location['longitude'], method='nearest')\n",
    "\n",
    "\"\"\" \n",
    "u10 and v10 form two sides of a right angled triangle so we can calculate\n",
    "the wind speed using the A^2 + B^2 = C^2 (Pythagoras, 530 BCE). With the \n",
    "wind variables this would be: u10^2 + v10^2 = windspeed^2. So this can be \n",
    "reconfigured as wind speed = the squareroot of (u10² + v10²).\n",
    "\"\"\"\n",
    "# Wind speed calculation\n",
    "wind_speed = np.sqrt(u10**2 + v10**2)\n",
    "\n",
    "# Extract wind speed value\n",
    "wind_speed_value = wind_speed.values.item() \n",
    "print(f\"Wind Speed at 10:00 on {date}: {wind_speed_value:.2f} m/s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3930168-7e93-4e4c-9f83-011f6f6ea8c7",
   "metadata": {},
   "source": [
    "## Running the tagged plume analysis\n",
    "\n",
    "The next code box analyses methane plumes we tagged earlier and provides the following information:\n",
    "\n",
    "- **Plume Insights**: Locations, sizes, and predicted methane emission rates (kg/h) visualised on an interactive map and summarised in a table.\n",
    "- **Model Evaluation**: Details on the regression model used to estimate emissions, including its performance metrics.\n",
    "\n",
    "There are potentially to variables that can be anjusted should the model not identify plumes adequately. Firstly \"window size\" which is the size of the search box around the coordinate you selected. If a bright non-plume object is being picked up instead of the plume you clicked this can be reduced. \n",
    "\n",
    "        # This defines a 50 pixel by 50 pixel search box\n",
    "        window_size = 50  <---- change this if needed  \n",
    "\n",
    "Secondly, for a plume to be counted as a plume a sufficent number of high value pixels need to be adjacent to one another. A cluster size of 60 may mean very small plumes are not picked up by the code. You can reduce the \"min_cluster_size\" to deal with this but the system will misidentify more features. \n",
    "\n",
    "        # Set a minimum threshold for a valid plume detection\n",
    "        min_cluster_size = 60  <---- change this if needed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8ddc9d-9dfa-4b93-a935-df53d8ceb5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "swir_diff_path = r'C:\\GIS_Course\\Methane_Point_Detection\\Sentinel-2_Algeria_Methane\\SWIR_diff_masked_urban.tiff'\n",
    "\n",
    "# Function to open the swir_diff_path file for analysis.\n",
    "with rasterio.open(swir_diff_path) as tiff_file:\n",
    "    raster_data = tiff_file.read(1)  # Read the first band\n",
    "    bounds = tiff_file.bounds\n",
    "    transform = tiff_file.transform\n",
    "\n",
    "    # Define nodata_value properly\n",
    "    nodata_value = tiff_file.nodata  # Extract from metadata if available\n",
    "    if nodata_value is None:\n",
    "        nodata_value = -32768  \n",
    "\n",
    "# Converts raster data into a masked array and hides the no data pixels.\n",
    "masked_data = np.ma.masked_equal(raster_data, nodata_value)\n",
    "\n",
    "# Compute min and max from remaining pixels.\n",
    "lower_bound = masked_data.min()  \n",
    "upper_bound = masked_data.max()  \n",
    "normalized_data = (masked_data - lower_bound) / (upper_bound - lower_bound)\n",
    "\n",
    "# function to centre the folium map on the raster\n",
    "def get_raster_centre(tiff_path):\n",
    "    with rasterio.open(tiff_path) as tiff_file:\n",
    "        bounds = tiff_file.bounds\n",
    "        centre_lat = (bounds.top + bounds.bottom) / 2\n",
    "        centre_lon = (bounds.left + bounds.right) / 2\n",
    "    return centre_lat, centre_lon\n",
    "\n",
    "\"\"\" \n",
    "This function sets up a bresenham algorithm to find all the pixel \n",
    "coordinates that form a straight line between two points. \n",
    "    \n",
    "    x0, y0: Start point coordinates.\n",
    "    x1, y1: End point coordinates.\n",
    "\n",
    "It then records the values of each of those pixels. \n",
    "\"\"\"\n",
    "def bresenham_line(x0, y0, x1, y1):\n",
    "    points = []  # Stores the pixels forming the line\n",
    "    dx = abs(x1 - x0)\n",
    "    dy = abs(y1 - y0)\n",
    "    \n",
    "    # Determine movement direction (+1 or -1 for each axis)\n",
    "    step_x = 1 if x0 < x1 else -1\n",
    "    step_y = 1 if y0 < y1 else -1\n",
    "    \n",
    "    # keeps track of how far the current point is from the ideal straight line\n",
    "    err = dx - dy\n",
    "\n",
    "    while (x0, y0) != (x1, y1):  # Stop when reaching the endpoint\n",
    "        points.append((x0, y0))  # Store the current position's value\n",
    "        \n",
    "        # double_err is used to decide whether to move horizontally, vertically, or diagonally.\n",
    "        double_err = err * 2 # \n",
    "        if double_err > -dy:\n",
    "            err -= dy\n",
    "            x0 += step_x\n",
    "        if double_err < dx:\n",
    "            err += dx\n",
    "            y0 += step_y\n",
    "\n",
    "    points.append((x1, y1)) \n",
    "    return points\n",
    "    \n",
    "# This function takes the bresenham_line and then applies it to the masked dataset.\n",
    "def get_line_pixel_values(start, end, masked_data):\n",
    "    line_pixels = bresenham_line(int(start[0]), int(start[1]), int(end[0]), int(end[1]))\n",
    "    pixel_values = [masked_data[row, col] for row, col in line_pixels if 0 <= row < masked_data.shape[0] and 0 <= col < masked_data.shape[1]]\n",
    "    return pixel_values, line_pixels\n",
    "\n",
    "\"\"\" calculate_plume_dimensions determines:\n",
    "    - plume_length: The longest distance between any two points in the plume (float).\n",
    "    - plume_width: The perpendicular width of the plume using PCA (float).\"\"\"\n",
    "def calculate_plume_dimensions(plume_pixels):\n",
    "    # Compute the convex hull\n",
    "    hull = ConvexHull(plume_pixels)\n",
    "    hull_points = plume_pixels[hull.vertices]\n",
    "\n",
    "    # Compute the plume length as the maximum pairwise distance between hull points\n",
    "    plume_length = pdist(hull_points).max()\n",
    "\n",
    "    # Use PCA to determine the major axis of the plume\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(plume_pixels)\n",
    "    main_direction = pca.components_[0]\n",
    "    perp_direction = np.array([-main_direction[1], main_direction[0]])  # Perpendicular vector\n",
    "\n",
    "    # Project plume pixels onto the perpendicular vector to compute width\n",
    "    projections = plume_pixels @ perp_direction\n",
    "    plume_width = projections.max() - projections.min()\n",
    "\n",
    "    return plume_length, plume_width\n",
    "\n",
    "\"\"\" analyze_plume_with_cross_section_sum performs the main plume analysis. For each\n",
    "tagged plume it will return its:\n",
    "    - C/S Sum\n",
    "    - Length\n",
    "    - Width\n",
    "    - emission rate (Q)\n",
    "    \n",
    "It also creates the folium map and a dataframe to show the data\"\"\"\n",
    "def analyze_plume_with_cross_section_sum(masked_data, plume_coords, transform, initial_centre):\n",
    "    # Folium map perameters\n",
    "    plume_map = folium.Map(location=initial_centre, zoom_start=11, control_scale=True)\n",
    "    \n",
    "    plume_results = [] # A place to store the plume results.\n",
    "\n",
    "    \"\"\"Labeled_array identifies plume regions using an absolute SWIR threshold \n",
    "    (`absolute_threshold`). This ensures consistent detection across different \n",
    "    scenes, independent of pixel distribution. Adjacent pixels are then grouped \n",
    "    and labeled as individual plumes.\"\"\"\n",
    "    absolute_threshold = 0.009  # pixels at or above this value are marked as plumes\n",
    "    labeled_array, _ = label(masked_data > absolute_threshold)\n",
    "\n",
    "    # Loops through each suspected plume location.\n",
    "    for i, (lat, lon) in enumerate(plume_coords):\n",
    "        try:\n",
    "            # Convert Lat/Long to raster grid coordinates.\n",
    "            row, col = rasterio.transform.rowcol(transform, lon, lat)\n",
    "            row, col = int(row), int(col)\n",
    "\n",
    "            # This defines a 50 pixel by 50 pixel search box around the plume coordinate \n",
    "            window_size = 50  \n",
    "            half_window = window_size // 2  \n",
    "\n",
    "            # Get the boundary of the search area in row/col coordinates\n",
    "            row_start, row_end = max(0, row - half_window), min(masked_data.shape[0], row + half_window + 1)\n",
    "            col_start, col_end = max(0, col - half_window), min(masked_data.shape[1], col + half_window + 1)\n",
    "\n",
    "            # Check if a plume is within the search box.\n",
    "            plume_label = labeled_array[row, col] \n",
    "\n",
    "            # Extract a local window of pixels around (row, col)\n",
    "            row_start, row_end = max(0, row - half_window), min(masked_data.shape[0], row + half_window + 1)\n",
    "            col_start, col_end = max(0, col - half_window), min(masked_data.shape[1], col + half_window + 1)\n",
    "            local_window = labeled_array[row_start:row_end, col_start:col_end]\n",
    "\n",
    "            \"\"\"Because of the slightly noisy quality of the data, individual pixels might\n",
    "            trigger the plume detection code. To minimse the risk of this the following code\n",
    "            specifies that 20 adjacent plume pixels are required to be counted as a plume. \"\"\"\n",
    "            \n",
    "            # Identify groups of connected plume pixels\n",
    "            binary_window = (local_window > 0).astype(int)  # Convert plume labels to binary (1 = plume, 0 = no plume)\n",
    "            labeled_clusters, num_clusters = label(binary_window)  # Label connected components\n",
    "\n",
    "            # Find the largest connected cluster size in the search box\n",
    "            cluster_sizes = np.bincount(labeled_clusters.ravel())[1:]  # Ignore background (index 0)\n",
    "            largest_cluster_size = cluster_sizes.max() if len(cluster_sizes) > 0 else 0\n",
    "\n",
    "            # Set a minimum threshold for a valid plume detection\n",
    "            min_cluster_size = 20  # This could be changed to a higher value if false postives persist. \n",
    "\n",
    "            if largest_cluster_size >= min_cluster_size:\n",
    "                # in the event of an overlapping plume, this selects the one with the largest pixel area.\n",
    "                # the smaller plume will need to be tagged by the user\n",
    "                unique_labels, counts = np.unique(local_window[local_window > 0], return_counts=True)\n",
    "                plume_label = unique_labels[np.argmax(counts)] if len(unique_labels) > 0 else 0\n",
    "            else:\n",
    "                plume_label = 0  # No valid plume detected\n",
    "\n",
    "            if plume_label == 0:\n",
    "                plume_results.append({\"Plume\": i + 1,\"Location\": (lat, lon), \"Status\": \"No plume\"})\n",
    "                continue\n",
    "\n",
    "            # Extract plume pixels.\n",
    "            plume_region = labeled_array == plume_label\n",
    "            plume_pixels = np.column_stack(np.where(plume_region))\n",
    "\n",
    "            # Compute plume width & legnth.\n",
    "            plume_length, plume_width = calculate_plume_dimensions(plume_pixels)\n",
    "\n",
    "            # Compute the plume centroid to serve as the cross section point.\n",
    "            centroid = plume_pixels.mean(axis=0)\n",
    "\n",
    "            # Draw a perpendicular cross-section line.\n",
    "            pca = PCA(n_components=2)\n",
    "            pca.fit(plume_pixels)\n",
    "            perp_direction = [-pca.components_[0, 1], pca.components_[0, 0]]\n",
    "            \n",
    "            # Define the perpendicular line in pixel coordinates.\n",
    "            perp_line_coords = [\n",
    "                (centroid[0] - perp_direction[0] * plume_width / 2, centroid[1] - perp_direction[1] * plume_width / 2),\n",
    "                (centroid[0] + perp_direction[0] * plume_width / 2, centroid[1] + perp_direction[1] * plume_width / 2),\n",
    "            ]\n",
    "\n",
    "            # Convert line coordinates to latitude/longitude for mapping\n",
    "            perp_line_latlon = [\n",
    "                rasterio.transform.xy(transform, int(pt[0]), int(pt[1])) for pt in perp_line_coords\n",
    "            ]\n",
    "            \n",
    "            # Add the cross-sectional line to the Folium map\n",
    "            folium.PolyLine(\n",
    "                locations=[(lat, lon) for lon, lat in perp_line_latlon],\n",
    "                color=\"blue\",\n",
    "                weight=2,\n",
    "                popup=f\"Cross Section for Plume {i + 1}\"\n",
    "            ).add_to(plume_map)\n",
    "\n",
    "            # Get pixel values along the perpendicular line.\n",
    "            line_pixel_values, line_pixels = get_line_pixel_values(perp_line_coords[0], perp_line_coords[1], masked_data)\n",
    "            num_intersecting_pixels = len(line_pixels)\n",
    "\n",
    "            # Compute cross-sectional sum.\n",
    "            adjusted_sum = sum(abs(value) for value in line_pixel_values)\n",
    "\n",
    "            # Create a convex hull to outline the plume for visualisation.\n",
    "            hull = ConvexHull(plume_pixels)\n",
    "            hull_coords = [(plume_pixels[vertex][0], plume_pixels[vertex][1]) for vertex in hull.vertices]\n",
    "            hull_latlon = [rasterio.transform.xy(transform, int(pt[0]), int(pt[1])) for pt in hull_coords]\n",
    "\n",
    "            # Add the plume outline on the map with the site label\n",
    "            site_label = f\"Site {i + 1}\" if i < 11 else f\"User {i - 10}\"\n",
    "            folium.Polygon(\n",
    "                locations=[(lat, lon) for lon, lat in hull_latlon],\n",
    "                color=\"red\",\n",
    "                weight=3,\n",
    "                fill=True,\n",
    "                fill_opacity=0.2,\n",
    "                popup=site_label,  # Use site label instead of \"Plume X\"\n",
    "            ).add_to(plume_map)\n",
    "\n",
    "            plume_results.append({\n",
    "                \"Plume\": i + 1,\n",
    "                \"Location\": (lat, lon),\n",
    "                \"Width (px)\": num_intersecting_pixels if plume_label > 0 else 0,\n",
    "                \"C/S Sum\": adjusted_sum if plume_label > 0 else 0,\n",
    "                \"Length (px)\": plume_length if plume_label > 0 else 0,\n",
    "                \"Q (kg/h)\": None,  # Placeholder for emission rate\n",
    "                \"Status\": \"Detected\" if plume_label > 0 else \"No plume\",\n",
    "            })\n",
    "\n",
    "  \n",
    "        # If a plume measurement encounters an error, this will allow any other plumes to be measured.\n",
    "        except Exception as e:\n",
    "            plume_results.append({\"Plume\": i + 1, \"Location\": (lat, lon), \"Status\": f\"Error: {e}\"})\n",
    "    \n",
    "    # Return measured plume stats and folium map.    \n",
    "    return plume_results, plume_map\n",
    "\n",
    "\"\"\" add_swir_data_to_map loads, normalizes, and adds a Short-Wave \n",
    "Infrared (SWIR) raster layer to an interactive Folium map.\"\"\"\n",
    "def add_swir_data_to_map(map_object, tiff_path):\n",
    "    with rasterio.open(tiff_path) as tiff_file: \n",
    "        swir_data = tiff_file.read(1)\n",
    "        bounds = tiff_file.bounds\n",
    "        nodata_value = -32768.0  # NaN value to be ignored. \n",
    "\n",
    "    # Mask NaN pixels and normalise data.\n",
    "    masked_data = np.ma.masked_equal(swir_data, nodata_value)\n",
    "    filtered_masked_data = masked_data[masked_data >= -3000]  # Ignore values below -3000\n",
    "    mean, std = np.nanmean(filtered_masked_data), np.nanstd(filtered_masked_data)\n",
    "    lower_bound, upper_bound = mean - std_factor * std, mean + std_factor * std\n",
    "    normalized_data = (masked_data - lower_bound) / (upper_bound - lower_bound)\n",
    "    normalized_data = np.clip(normalized_data, 0, 1)\n",
    "\n",
    "    # Convert SWIR data to RGB for mapping.\n",
    "    cmap = plt.get_cmap(\"viridis\")\n",
    "    swir_rgb = (cmap(normalized_data)[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_bounds = [[bounds.bottom, bounds.left], [bounds.top, bounds.right]]\n",
    "\n",
    "    # Add SWIR overlay to the map.\n",
    "    swir_overlay = ImageOverlay(\n",
    "        name=\"SWIR Data\",\n",
    "        image=swir_rgb,\n",
    "        bounds=image_bounds,\n",
    "        opacity=1,  # Match Truecolour opacity\n",
    "        interactive=True,  # Allow user interaction\n",
    "        zindex=2  # Place above Truecolour\n",
    "    )\n",
    "    swir_overlay.add_to(map_object)\n",
    "\n",
    "# Convert the dataset into a DataFrame\n",
    "model_df = pd.DataFrame(initial_data)\n",
    "\n",
    "\"\"\" Update_model trains and evaluates a XGBoost regression model \n",
    "to predict methane emission rates based on the known plume data shown\n",
    "earlier.\"\"\"\n",
    "\n",
    "def update_model(df):\n",
    "    # Extract independent variables (X) and dependent variable (y)\n",
    "    X = df[[\"Cross_sectional_Adjusted_Sum\", \"Wind_speed\", \"Plume_length\", \"Width\"]]  \n",
    "    y = df[\"Emission_rate_kg_h\"]\n",
    "\n",
    "    # Apply log transformation to y correctly\n",
    "    y_log = np.log(y + 1)  # Adding 1 to prevent log(0) errors\n",
    "\n",
    "    # Standardize the features properly\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Train XGBoost model with optimized settings\n",
    "    xgb_model = XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=5,\n",
    "        colsample_bytree=0.8,\n",
    "        subsample=0.8,\n",
    "        gamma=0.1,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=0.1,\n",
    "        random_state=42 # what is the meaning of Life the Universe and Everything?\n",
    "    )\n",
    "\n",
    "    # Train the model on the log-transformed y\n",
    "    xgb_model.fit(X_scaled, y_log)\n",
    "\n",
    "    return X_scaled, y, scaler, xgb_model  \n",
    "\n",
    "# Train the improved XGBoost model\n",
    "X_scaled, y, scaler, xgb_model = update_model(model_df)\n",
    "\n",
    "# Get the centre of the SWIR TIFF\n",
    "centre_coords = get_raster_centre(swir_diff_path)\n",
    "\n",
    "# Perform plume analysis, centreing the map on the SWIR TIFF\n",
    "plume_analysis_results, plume_map = analyze_plume_with_cross_section_sum(masked_data, plume_coords, transform, centre_coords)\n",
    "\n",
    "# Add wind speed to each plume analysis result\n",
    "for plume in plume_analysis_results:\n",
    "    if plume[\"Status\"] == \"Detected\":  # Only predict for detected plumes\n",
    "        adjusted_sum = plume[\"C/S Sum\"]\n",
    "        plume_length = plume[\"Length (px)\"]\n",
    "        width_value = plume[\"Width (px)\"]  # Extract width from the plume data\n",
    "\n",
    "        # Convert input features to a DataFrame to maintain feature names\n",
    "        input_features = pd.DataFrame([[adjusted_sum, wind_speed_value, plume_length, width_value]], \n",
    "                                      columns=[\"Cross_sectional_Adjusted_Sum\", \"Wind_speed\", \"Plume_length\", \"Width\"])\n",
    "        # Scale the input data using the same scaler\n",
    "        input_features_scaled = scaler.transform(input_features)\n",
    "        # Use the trained XGBoost model for prediction\n",
    "        log_prediction = xgb_model.predict(input_features_scaled)\n",
    "        # Convert back from log scale\n",
    "        emission_rate = np.exp(log_prediction[0]) - 1\n",
    "        # Store the emission rate\n",
    "        plume[\"Predicted Emission Rate (kg/h)\"] = emission_rate\n",
    "\n",
    "# Convert updated results to a DataFrame\n",
    "plume_df = pd.DataFrame(plume_analysis_results)\n",
    "# Rename the \"Plume\" index to \"Site X\" or \"User X\"\n",
    "plume_labels = [f\"Site {i + 1}\" if i < 11 else f\"User {i - 10}\" for i in range(len(plume_df))]\n",
    "\n",
    "# Set the custom labels as the new index\n",
    "plume_df.index = plume_labels\n",
    "\n",
    "plume_df = plume_df.rename(columns={\n",
    "    \"Cross_sectional_Adjusted_Sum\": \"C/S Sum\",\n",
    "    \"Plume_length\": \"Length (px)\",\n",
    "    \"Width\": \"Width (px)\",\n",
    "    \"Predicted Emission Rate (kg/h)\": \"Q (kg/h)\"\n",
    "})\n",
    "\n",
    "# Dataframe column order\n",
    "column_order = [\"Status\", \"Location\", \"Width (px)\", \"C/S Sum\", \"Length (px)\", \"Q (kg/h)\"]\n",
    "existing_columns = [col for col in column_order if col in plume_df.columns]\n",
    "plume_df = plume_df[existing_columns]\n",
    "\n",
    "# Hiding the dataframe keyfield as it isn't needed\n",
    "if \"Plume\" in plume_df.columns: plume_df.drop(columns=[\"Plume\"], inplace=True)\n",
    "\n",
    "# Display updated DataFrame with predicted emission rates\n",
    "r_squared = xgb_model.score(X_scaled, np.log(model_df[\"Emission_rate_kg_h\"] + 1))\n",
    "print(f\"Model R-squared (R²): {r_squared:.4f}\")\n",
    "print(f\"Active Emission Date: {active_temporal_extent[0]}\")\n",
    "print(f\"No Emission Date: {no_temporal_extent[0]}\")\n",
    "print(f\"Wind Speed: {wind_speed_value:.2f} m/s\")\n",
    "pd.set_option(\"display.width\", 200)  #Controls dataframe width\n",
    "pd.set_option(\"display.max_columns\", None)  # \"None\" puts all a plume's results on one line\n",
    "# Replace NaN values with an empty string for readability\n",
    "plume_df = plume_df.fillna(\"\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(plume_df)\n",
    "\n",
    "# Load the true colour image\n",
    "truecolour_sat = 'Sentinel-2_truecolour_reprojected.Tiff'\n",
    "img = rasterio.open(truecolour_sat)\n",
    "blue, green, red = img.read(1), img.read(2), img.read(3)\n",
    "\n",
    "# Adjust brightness of truecolour image\n",
    "brightness_factor = 0.03 # only change this if the truecolour image is too dark or bright\n",
    "blue = np.clip(blue * brightness_factor, 0, 255)\n",
    "green = np.clip(green * brightness_factor, 0, 255)\n",
    "red = np.clip(red * brightness_factor, 0, 255)\n",
    "\n",
    "# Stack bands to create RGB image\n",
    "rgb = np.dstack((red, green, blue))\n",
    "rgb = rgb / rgb.max()\n",
    "rgb = np.log1p(rgb)\n",
    "rgb = rgb / rgb.max()\n",
    "\n",
    "# Add true colour image overlay\n",
    "truecolour_overlay = ImageOverlay(\n",
    "    name= \"Truecolour\",\n",
    "    image=rgb,\n",
    "    bounds=swir_bounds,\n",
    "    opacity=1,  # Lower opacity for blending with SWIR overlay\n",
    "    interactive=True,\n",
    "    zindex=1,  \n",
    ")\n",
    "truecolour_overlay.add_to(plume_map)\n",
    "\n",
    "# Add SWIR overlay to the map\n",
    "add_swir_data_to_map(plume_map, swir_diff_path)\n",
    "\n",
    "# Add a layer control to toggle map layers\n",
    "LayerControl().add_to(plume_map)\n",
    "\n",
    "# Display the map with updated analysis\n",
    "display(plume_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b74ded-f7d9-40f2-9a84-07db203bcdf5",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. **Gorroño, J., Varon, D.J., Irakulis-Loitxate, I. and Guanter, L., 2022.** Understanding the potential of Sentinel-2 for monitoring methane point emissions. *Atmospheric Measurement Techniques Discussions, 2022*, pp.1-25.\n",
    "\n",
    "2. **Pandey, S., van Nistelrooij, M., Maasakkers, J.D., Sutar, P., Houweling, S., Varon, D.J., Tol, P., Gains, D., Worden, J. and Aben, I., 2023.** Daily detection and quantification of methane leaks using Sentinel-3: a tiered satellite observation approach with Sentinel-2 and Sentinel-5p. *Remote Sensing of Environment, 296*, p.113716.\n",
    "\n",
    "3. **Pythagoras, 530 BCE.** A squared plus B squared equals C squared: The definitive guide to right angled triangles. Ancient Greece: Croton Press.\n",
    "\n",
    "4. **Naus, S., Maasakkers, J.D., Gautam, R., Omara, M., Stikker, R., Veenstra, A.K., Nathan, B., Irakulis-Loitxate, I., Guanter, L., Pandey, S. and Girard, M., 2023.** Assessing the relative importance of satellite-detected methane superemitters in quantifying total emissions for oil and gas production areas in algeria. Environmental Science & Technology, 57(48), pp.19545-19556.\n",
    "\n",
    "5. **Radman, A., Mahdianpari, M., Varon, D.J. and Mohammadimanesh, F., 2023.** S2MetNet: A novel dataset and deep learning benchmark for methane point source quantification using Sentinel-2 satellite imagery. *Remote Sensing of Environment, 295*, p.113708.\n",
    "\n",
    "6. **Varon, D.J., Jervis, D., McKeever, J., Spence, I., Gains, D. and Jacob, D.J., 2020.** High-frequency monitoring of anomalous methane point sources with multispectral Sentinel-2 satellite observations. *Atmospheric Measurement Techniques Discussions, 2020*, pp.1-21."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
